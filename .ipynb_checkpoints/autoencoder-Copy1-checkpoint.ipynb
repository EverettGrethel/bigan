{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c44b4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import cv2\n",
    "from PIL import Image as im\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "import collections\n",
    "from typing import DefaultDict, Tuple, List, Dict\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495aef96-9947-4968-a7ac-177f81a83dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('early-stopping-pytorch')\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "# Adjust printing view dimensions\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=300)\n",
    "torch.set_printoptions(threshold=sys.maxsize, linewidth=300, profile='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6b7c31-6d35-4067-ba20-89b11ac6b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_shape = kwargs[\"input_shape\"]\n",
    "        # number of hidden units in first hidden layer\n",
    "        self.n_units = kwargs[\"n_units\"]\n",
    "        # number of hidden units in latent space\n",
    "        self.latent_units = kwargs[\"latent_units\"]\n",
    "        \n",
    "        # ---- ENCODER ---- #\n",
    "        self.encoder_in = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=self.input_shape, out_features=self.n_units),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        # Bottleneck is actually in the encoder, but it must be isolated in order to calculate sparsity\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=self.n_units, out_features=self.latent_units),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        # ----------------- #\n",
    "        \n",
    "        # ---- DECODER ---- #\n",
    "        self.decoder_in = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_units, out_features=self.n_units),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        # Last layer is separate in order to use a different initialization (on account of sigmoid)\n",
    "        self.decoder_out = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=self.n_units, out_features=self.input_shape),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        # ----------------- #\n",
    "    \n",
    "    # X denotes features\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder_in(X)\n",
    "        bottleneck = self.bottleneck(encoded)\n",
    "        decoded = self.decoder_in(bottleneck)\n",
    "        decoded_out = self.decoder_out(decoded)\n",
    "        return bottleneck, decoded_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca68d50-8eeb-4d3a-8150-f5508bdf9d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# 10,000 samples, 30x30 matrices\n",
    "data_count = 10000\n",
    "data = np.ndarray(shape=(data_count,30,30))\n",
    "n_features = data.shape[1] * data.shape[2]\n",
    "\n",
    "\n",
    "for i in range(data_count):\n",
    "    path = f'data/jet_matrices/sample{i+1}.dat'\n",
    "    sample = np.loadtxt(path, unpack = False)\n",
    "    data[i] = sample\n",
    "\n",
    "print(\"Done loading data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdfa233-d473-4cab-b350-a01415b07f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading parameters.\n"
     ]
    }
   ],
   "source": [
    "# Load parameters corresponding to the 4 variables input into \n",
    "# the Helmholtz Resonator function, where output is each sample in dataset.\n",
    "params = np.ndarray(shape=(data_count,4))\n",
    "\n",
    "path = r'data/param_lhs.dat'\n",
    "with open(path) as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i >= params.shape[0]:\n",
    "            break\n",
    "        param = np.fromstring(line, dtype=float, sep=',')\n",
    "        params[i] = param\n",
    "\n",
    "print(\"Done loading parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8891f7fc-06ec-4a54-9e54-3189e67da3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flatten data and convert to Torch Tensor\n",
    "\n",
    "# 10,000 samples, 900 features\n",
    "X = np.ndarray(shape=(data_count, n_features))\n",
    "for i, sample in enumerate(data):\n",
    "    if i >= X.shape[0]:\n",
    "        break\n",
    "    flat = sample.flatten()\n",
    "    X[i] = flat\n",
    "\n",
    "# Convert from numpy array to Pytorch tensor\n",
    "X = torch.from_numpy(X)\n",
    "# Convert all scalars to floats. May affect training behavior (ie. reconstructions made of non-binary scalar values)\n",
    "X = X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38354ca1-cb26-48dc-ad4b-73c616477bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_pca = False\n",
    "is_scramble = False\n",
    "\n",
    "def scramble_data(X):\n",
    "    print(\"scrambling data...\")\n",
    "    X_pert = np.copy(X)\n",
    "    i = 0\n",
    "    for col in X.T:\n",
    "        # print(col)\n",
    "        X_pert[:,i] = np.random.permutation(col)\n",
    "        # print(X_pert[:,i])\n",
    "        i += 1\n",
    "    return X_pert\n",
    "\n",
    "# Scramble data if using base model\n",
    "if is_scramble:\n",
    "    X = scramble_data(X)\n",
    "# Pair data samples with their corresponding parameter\n",
    "# in order to keep organized during random splitting.\n",
    "X_with_params = []\n",
    "for i in range(data_count):\n",
    "    pair = [X[i], params[i]]\n",
    "    X_with_params.append(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40435a6-d2d2-4a67-8b9d-a4ceeabda9f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f802-706f-4e61-b59a-1b5a7c5c11c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot cumulative explained variance w.r.t. number of components\n",
    "\n",
    "def pca_run(X):\n",
    "    pca = PCA(n_components=0.95).fit(X)\n",
    "\n",
    "    #% matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    y = np.cumsum(pca.explained_variance_ratio_)\n",
    "    # n_components = number of components needed to reach cum. variance threshold\n",
    "    n_components = y.size\n",
    "    xi = np.arange(1, n_components+1, step=1)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    #change from 0-based array index to 1-based human-readable label\n",
    "    plt.xticks(np.arange(0, n_components+1, step=1))\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The Number of Components Needed to Explain Variance')\n",
    "\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.axhline(y=0.8, color='g', linestyle='-')\n",
    "    plt.axhline(y=0.9, color='b', linestyle='-')\n",
    "    plt.text(0, 0.915, '95% cut-off threshold', color = 'red', fontsize=13)\n",
    "    plt.text(24, 0.85, '90% cut-off threshold', color = 'blue', fontsize=13)\n",
    "    plt.text(12, 0.75, '80% cut-off threshold', color = 'green', fontsize=13)\n",
    "\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()\n",
    "\n",
    "# Run with original data.\n",
    "pca_run(X.numpy())\n",
    "\n",
    "# Run with permutated data.\n",
    "# De-correlates features, so performing worse than original data indicates\n",
    "# existence of correlation in the original data's features.\n",
    "X_pert = scramble_data(X)\n",
    "pca_run(X_pert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75306b-57ab-407a-bb35-d7f160e75c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(xi, pca.explained_variance_ratio_, width=0.4)\n",
    "plt.ylabel(\"Percent of Total Variance\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.title(\"Significance of Each Principal Component Towards Variance \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96362bf4-42d8-49b8-a6fa-aeb5a8359ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "# Toggle to indicate to training that PCA is in use\n",
    "is_pca = True\n",
    "# -- DEFINE NUMBER OF COMPONENTS HERE --\n",
    "n_components = 5\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(X.numpy())\n",
    "\n",
    "print(X)\n",
    "# If fails, re-run \"Flatten data...\" cell\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca = torch.from_numpy(X_pca)\n",
    "# Convert all scalars to floats. May affect training behavior (ie. reconstructions made of non-binary scalar values)\n",
    "X_pca = X_pca.float()\n",
    "# Replace former n_features with number of components\n",
    "n_features = X_pca.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875d4ed-f4c5-4bf5-878e-ccb76d4645d7",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc40b83a-dcd0-457f-9b37-0261177fb051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# Changes X based on whether PCA was used\n",
    "X_2 = None\n",
    "if is_pca:\n",
    "    X_2 = X_pca\n",
    "else:\n",
    "    X_2 = X\n",
    "\n",
    "batch_size = 32\n",
    "# 70/15/15 split\n",
    "train_size = int(0.7 * len(X_2))\n",
    "val_test_size = len(X_2) - train_size\n",
    "test_size = val_test_size // 2\n",
    "    \n",
    "val_size = val_test_size - test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d9f72fd-043f-4203-ba7e-3098c76b6a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initate data loaders\n",
    "\n",
    "train, val = torch.utils.data.random_split(X_with_params, [train_size, val_test_size], generator=torch.Generator().manual_seed(5))\n",
    "val, test = torch.utils.data.random_split(val, [val_size, test_size], generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# Same as test_loader but with stochastic batch size\n",
    "test_loader_stoch = torch.utils.data.DataLoader(\n",
    "    test, batch_size=1, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b397d61-b904-4254-a10b-45702e3f6f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "#       TRAINING & VALIDATION       #\n",
    "#####################################\n",
    "\n",
    "def kl_divergence(p, p_hat):\n",
    "    funcs = nn.Sigmoid()\n",
    "    p_hat = torch.mean(funcs(p_hat), 1)\n",
    "    #p_hat = torch.nn.functional.softmax(p_hat, dim=1)\n",
    "    p_tensor = torch.Tensor([p] * len(p_hat)).to(device)\n",
    "    #p_tensor = torch.nn.functional.softmax(p_tensor, dim=1)\n",
    "    \n",
    "    log_p_hat = torch.log(p_hat)\n",
    "    log_p_tensor = torch.log(p_tensor)\n",
    "    log_one_minus_p_hat = torch.log(1 - p_hat)\n",
    "    log_one_minus_p_tensor = torch.log(1 - p_tensor)\n",
    "    # print(f'p_hat = {p_hat}')\n",
    "    # print(f'log_p_hat = {log_p_hat}')\n",
    "    # print(f'log_p_tensor = {log_p_tensor}')\n",
    "    # print(f'log_one_minus_p_hat = {log_one_minus_p_hat}')\n",
    "    # print(f'log_one_minus_p_tensor = {log_one_minus_p_tensor}')\n",
    "    \n",
    "    # FORMULA: p*log(p) - p*log(p_hat) + (1-p)*log(1-p) - (1-p)*log(1-p_hat)\n",
    "    return torch.sum(p_tensor * log_p_tensor - p_tensor * log_p_hat + (1 - p_tensor) * log_one_minus_p_tensor - (1 - p_tensor) * log_one_minus_p_hat)\n",
    "\n",
    "\n",
    "# Computes sparsity loss term for latent layer (ie. bottleneck) only.\n",
    "def sparse_loss_latent(model, rho, data):\n",
    "    loss = 0\n",
    "    # As rho_hat is passed through each layer, it is redefined as that layer's output.\n",
    "    rho_hat = data\n",
    "    \n",
    "    enc_layer = list(model.encoder_in.children())[0]\n",
    "    enc_relu = list(model.encoder_in.children())[1]\n",
    "    rho_hat = enc_layer(rho_hat)\n",
    "    rho_hat = enc_relu(rho_hat)\n",
    "    \n",
    "    btl_layer = list(model.bottleneck.children())[0]\n",
    "    btl_relu = list(model.bottleneck.children())[1]\n",
    "    rho_hat = btl_layer(rho_hat)\n",
    "    rho_hat = btl_relu(rho_hat)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Computes sparsity loss term for all layers.\n",
    "def sparse_loss_all(model, rho, data):\n",
    "    loss = 0\n",
    "    # As rho_hat is passed through each layer, it is redefined as that layer's output.\n",
    "    rho_hat = data\n",
    "    \n",
    "    enc_layer = list(model.encoder_in.children())[0]\n",
    "    enc_relu = list(model.encoder_in.children())[1]\n",
    "    rho_hat = enc_layer(rho_hat)\n",
    "    rho_hat = enc_relu(rho_hat)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "    \n",
    "    btl_layer = list(model.bottleneck.children())[0]\n",
    "    btl_relu = list(model.bottleneck.children())[1]\n",
    "    rho_hat = btl_layer(rho_hat)\n",
    "    rho_hat = btl_relu(rho_hat)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "\n",
    "    dec_in_layer = list(model.decoder_in.children())[0]\n",
    "    dec_in_relu = list(model.decoder_in.children())[1]\n",
    "    rho_hat = dec_in_layer(rho_hat)\n",
    "    rho_hat = dec_in_relu(rho_hat)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "    \n",
    "    dec_out_layer = list(model.decoder_out.children())[0]\n",
    "    # We skip the activation in this layer because it is Sigmoid which is already computed in the kl_divergence function.\n",
    "    rho_hat = dec_out_layer(rho_hat)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "class TrainedModel():\n",
    "    def __init__(self, model, avg_train_loss, avg_val_loss, epochs):\n",
    "        self.model = model\n",
    "        self.avg_train_loss = avg_train_loss\n",
    "        self.avg_val_loss = avg_val_loss\n",
    "        self.epochs = epochs\n",
    "\n",
    "        \n",
    "# Training and Validation are combined in order to allow for early stopping\n",
    "def train_validate(model, epochs, lr, is_early_stopping=False, is_pca=False, is_sparse=False, patience=None, beta=None, rho=None):\n",
    "    # Define Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Binary Cross Entropy Loss\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Reset model state\n",
    "    def weights_init(m):\n",
    "        if type(m) == nn.Sequential:\n",
    "            children = dict(m.named_children())\n",
    "            # Use Kaiming if ReLU, Xavier if Sigmoid.\n",
    "            if type(children['1']) == nn.ReLU:\n",
    "                nn.init.kaiming_uniform_(children['0'].weight.data, nonlinearity='relu')\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(children['0'].weight.data)\n",
    "            \n",
    "    model.apply(weights_init)\n",
    "    \n",
    "    # Get average initial latent layer weights\n",
    "    avg_weights = []\n",
    "    for neuron in model.state_dict()['bottleneck.0.weight']:\n",
    "        avg = torch.mean(neuron)\n",
    "        avg_weights.append(avg.item())\n",
    "    print(\"Initialized Latent Layer Weights:\")\n",
    "    print(avg_weights)\n",
    "\n",
    "    # Toggle Early Stopping (if using).\n",
    "    if is_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "        print(\"Using Early Stopping\")\n",
    "    if is_pca:\n",
    "        print(\"Using PCA\")\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #############################    \n",
    "        #          TRAINING         #\n",
    "        #############################\n",
    "        \n",
    "        loss = 0\n",
    "        # Prepare model for training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            # remove params, keep data\n",
    "            batch = batch[0]\n",
    "            # reshape mini-batch data from [batch_size, 30, 30] to [batch_size, 900]\n",
    "            # load it to the active device\n",
    "            batch = batch.view(-1, n_features).to(device)\n",
    "\n",
    "            # reset the gradients back to zero\n",
    "            # PyTorch accumulates gradients on subsequent backward passes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute reconstructions\n",
    "            # also retrieve bottleneck weights for computing sparsity penalty\n",
    "            bottleneck, decoded = model(batch)\n",
    "\n",
    "            # Exception handler for when BCE loss has values outside range [0.0, 1.0]\n",
    "            try:\n",
    "                # Compute training reconstruction loss\n",
    "                train_loss = criterion(decoded, batch)\n",
    "            except RuntimeError:\n",
    "                print('Runtime Error during loss calculation. BCE loss has values outside range [0.0, 1.0]')\n",
    "                for k, sample in enumerate(decoded):\n",
    "                    print(k)\n",
    "                    print(sample)\n",
    "                    \n",
    "            # Add sparsity penalty to loss, if toggled\n",
    "            if is_sparse:\n",
    "                sparsities = []\n",
    "                kl_loss = sparse_loss_latent(model, rho, batch)\n",
    "                sparsity_penalty = beta * kl_loss\n",
    "                train_loss = train_loss + sparsity_penalty\n",
    "                sparsities.append(sparsity_penalty.item())\n",
    "                # print(f'sparsity: {sparsity_penalty}\\n')\n",
    "                # print(f'training loss: {train_loss}\\n')\n",
    "                \n",
    "                # Check whether KL divergence is behaving correctly (ie. should be nonnegative).\n",
    "                if torch.all(sparsity_penalty < 0):\n",
    "                    print('Training Error: sparsity penalty is negative.')\n",
    "                    print(f'sparsity: {sparsity_penalty}\\n')\n",
    "                    print(f'training loss: {train_loss}\\n')\n",
    "                \n",
    "#                 # Apply sparsity term only to latent layer (bottleneck).\n",
    "#                 for name, module in model.named_modules():\n",
    "#                     # Get all layers, excluding everything that is not a layer.\n",
    "#                     if not isinstance(module, (nn.Sequential, AutoEncoder, torch.nn.ReLU, torch.nn.Sigmoid)):\n",
    "#                         if name == 'bottleneck.0':\n",
    "#                             for parameter in module.parameters():\n",
    "#                                 parameter.requires_grad = True\n",
    "#                         else:\n",
    "#                             for parameter in module.parameters():\n",
    "#                                  parameter.requires_grad = False\n",
    "\n",
    "#                 # Compute gradient just for latent layer.\n",
    "#                 train_loss_reg.backward(retain_graph=True)\n",
    "                \n",
    "#                 sparsities.append(sparsity_penalty.item())\n",
    "                \n",
    "#                 # Re-enable non-latent layer gradients and disable latent gradient.\n",
    "#                 for name, module in model.named_modules():\n",
    "#                     if not isinstance(module, (nn.Sequential, AutoEncoder, torch.nn.ReLU, torch.nn.Sigmoid)):\n",
    "#                         if name == 'bottleneck.0':\n",
    "#                             for parameter in module.parameters():\n",
    "#                                  parameter.requires_grad = False\n",
    "#                         else:\n",
    "#                             for parameter in module.parameters():\n",
    "#                                  parameter.requires_grad = True\n",
    "            \n",
    "            # If basic, then compute gradient for all layers.\n",
    "            # If sparse, then compute gradients for every layer except latent layer (bottleneck).\n",
    "            train_loss.backward()\n",
    "            \n",
    "            # Perform parameter update based on current gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the mini-batch training loss to epoch loss\n",
    "            train_losses.append(train_loss.item())\n",
    "\n",
    "        # Compute the epoch training loss\n",
    "        avg_train_loss = np.average(train_losses)\n",
    "        if is_sparse:\n",
    "            avg_sparsity = np.average(sparsities)\n",
    "\n",
    "        #############################    \n",
    "        #         VALIDATION        #\n",
    "        #############################\n",
    "\n",
    "        # Decoupled into three lists due to issue with placing torch tensors into multidimensional lists\n",
    "        batches = []\n",
    "        recons = []\n",
    "        val_losses = []\n",
    "\n",
    "        # Prepare model for evaluation\n",
    "        model.eval()\n",
    "\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader, 0):\n",
    "                # remove params, keep data \n",
    "                batch = batch[0]\n",
    "                batch = batch.view(-1, n_features).to(device)\n",
    "                bottleneck, reconstructions = model(batch)\n",
    "                # Reconstruction loss\n",
    "                val_loss = criterion(reconstructions, batch)\n",
    "                \n",
    "                # Store samples, predictions, and loss for visualization purposes\n",
    "                batches.append(batch)\n",
    "                recons.append(reconstructions)\n",
    "                val_losses.append(val_loss.item())\n",
    "                #print(f'Batch {i}: {val_loss.item()}')\n",
    "\n",
    "        avg_val_loss = np.average(val_losses)\n",
    "        \n",
    "        # display the epoch training loss and validation loss\n",
    "        print(\"Epoch : {}/{}, Training Loss = {:.6f}, Validation Loss = {:.6f}\".format(epoch + 1, epochs, avg_train_loss, avg_val_loss))\n",
    "        if is_sparse:\n",
    "            print(f\"Average Sparsity = {avg_sparsity}\")\n",
    "        \n",
    "        opt_epochs = epochs\n",
    "        \n",
    "        if is_early_stopping:\n",
    "            early_stopping(avg_val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                opt_epochs = epoch + 1\n",
    "                print(\"Early stopping...\")\n",
    "                # Exit training loop\n",
    "                break\n",
    "    \n",
    "    if not is_early_stopping:\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    avg_weights = []\n",
    "    for neuron in model.state_dict()['bottleneck.0.weight']:\n",
    "        avg = torch.mean(neuron)\n",
    "        avg_weights.append(avg.item())\n",
    "    print(avg_weights)\n",
    "    print(f\"Epochs: {opt_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "    trained_model = TrainedModel(model, avg_train_loss, avg_val_loss, epochs)\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dfe00d81-b453-42f0-9960-31b097315258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Latent Layer Weights:\n",
      "[0.007588679436594248, -0.012624192982912064, 0.010443619452416897, 0.021243233233690262, -0.05041636526584625, 0.009892481379210949, 0.018681535497307777, 0.03931600600481033, 0.04897252097725868, -0.04136284813284874]\n",
      "Training...\n",
      "Epoch : 1/100, Training Loss = 0.089466, Validation Loss = 0.032391\n",
      "Epoch : 2/100, Training Loss = 0.027187, Validation Loss = 0.019458\n",
      "Epoch : 3/100, Training Loss = 0.014427, Validation Loss = 0.011587\n",
      "Epoch : 4/100, Training Loss = 0.010965, Validation Loss = 0.009860\n",
      "Epoch : 5/100, Training Loss = 0.009686, Validation Loss = 0.009044\n",
      "Epoch : 6/100, Training Loss = 0.008787, Validation Loss = 0.008608\n",
      "Epoch : 7/100, Training Loss = 0.008129, Validation Loss = 0.007968\n",
      "Epoch : 8/100, Training Loss = 0.007560, Validation Loss = 0.007395\n",
      "Epoch : 9/100, Training Loss = 0.007091, Validation Loss = 0.007058\n",
      "Epoch : 10/100, Training Loss = 0.006667, Validation Loss = 0.006503\n",
      "Epoch : 11/100, Training Loss = 0.006286, Validation Loss = 0.006101\n",
      "Epoch : 12/100, Training Loss = 0.005915, Validation Loss = 0.005873\n",
      "Epoch : 13/100, Training Loss = 0.005677, Validation Loss = 0.005511\n",
      "Epoch : 14/100, Training Loss = 0.005301, Validation Loss = 0.005197\n",
      "Epoch : 15/100, Training Loss = 0.005050, Validation Loss = 0.005237\n",
      "Epoch : 16/100, Training Loss = 0.004854, Validation Loss = 0.004946\n",
      "Epoch : 17/100, Training Loss = 0.004672, Validation Loss = 0.004787\n",
      "Epoch : 18/100, Training Loss = 0.004558, Validation Loss = 0.004687\n",
      "Epoch : 19/100, Training Loss = 0.004360, Validation Loss = 0.004640\n",
      "Epoch : 20/100, Training Loss = 0.004261, Validation Loss = 0.004377\n",
      "Epoch : 21/100, Training Loss = 0.004131, Validation Loss = 0.004313\n",
      "Epoch : 22/100, Training Loss = 0.003987, Validation Loss = 0.004067\n",
      "Epoch : 23/100, Training Loss = 0.003964, Validation Loss = 0.004125\n",
      "Epoch : 24/100, Training Loss = 0.003888, Validation Loss = 0.004001\n",
      "Epoch : 25/100, Training Loss = 0.003717, Validation Loss = 0.003846\n",
      "Epoch : 26/100, Training Loss = 0.003674, Validation Loss = 0.003636\n",
      "Epoch : 27/100, Training Loss = 0.003521, Validation Loss = 0.004061\n",
      "Epoch : 28/100, Training Loss = 0.003520, Validation Loss = 0.003679\n",
      "Epoch : 29/100, Training Loss = 0.003421, Validation Loss = 0.003703\n",
      "Epoch : 30/100, Training Loss = 0.003379, Validation Loss = 0.003421\n",
      "Epoch : 31/100, Training Loss = 0.003303, Validation Loss = 0.003570\n",
      "Epoch : 32/100, Training Loss = 0.003298, Validation Loss = 0.003560\n",
      "Epoch : 33/100, Training Loss = 0.003273, Validation Loss = 0.003488\n",
      "Epoch : 34/100, Training Loss = 0.003162, Validation Loss = 0.003230\n",
      "Epoch : 35/100, Training Loss = 0.003105, Validation Loss = 0.003379\n",
      "Epoch : 36/100, Training Loss = 0.003199, Validation Loss = 0.003168\n",
      "Epoch : 37/100, Training Loss = 0.003067, Validation Loss = 0.003101\n",
      "Epoch : 38/100, Training Loss = 0.002980, Validation Loss = 0.003060\n",
      "Epoch : 39/100, Training Loss = 0.002943, Validation Loss = 0.003152\n",
      "Epoch : 40/100, Training Loss = 0.002966, Validation Loss = 0.003028\n",
      "Epoch : 41/100, Training Loss = 0.002992, Validation Loss = 0.003156\n",
      "Epoch : 42/100, Training Loss = 0.002927, Validation Loss = 0.003237\n",
      "Epoch : 43/100, Training Loss = 0.002858, Validation Loss = 0.002956\n",
      "Epoch : 44/100, Training Loss = 0.002858, Validation Loss = 0.003080\n",
      "Epoch : 45/100, Training Loss = 0.002816, Validation Loss = 0.003137\n",
      "Epoch : 46/100, Training Loss = 0.002798, Validation Loss = 0.003237\n",
      "Epoch : 47/100, Training Loss = 0.002808, Validation Loss = 0.003195\n",
      "Epoch : 48/100, Training Loss = 0.002807, Validation Loss = 0.003130\n",
      "Epoch : 49/100, Training Loss = 0.002713, Validation Loss = 0.002846\n",
      "Epoch : 50/100, Training Loss = 0.002692, Validation Loss = 0.002774\n",
      "Epoch : 51/100, Training Loss = 0.002696, Validation Loss = 0.002898\n",
      "Epoch : 52/100, Training Loss = 0.002680, Validation Loss = 0.003032\n",
      "Epoch : 53/100, Training Loss = 0.002653, Validation Loss = 0.002913\n",
      "Epoch : 54/100, Training Loss = 0.002592, Validation Loss = 0.003076\n",
      "Epoch : 55/100, Training Loss = 0.002561, Validation Loss = 0.003200\n",
      "Epoch : 56/100, Training Loss = 0.002636, Validation Loss = 0.002795\n",
      "Epoch : 57/100, Training Loss = 0.002544, Validation Loss = 0.002722\n",
      "Epoch : 58/100, Training Loss = 0.002565, Validation Loss = 0.002861\n",
      "Epoch : 59/100, Training Loss = 0.002439, Validation Loss = 0.002983\n",
      "Epoch : 60/100, Training Loss = 0.002445, Validation Loss = 0.002914\n",
      "Epoch : 61/100, Training Loss = 0.002412, Validation Loss = 0.002755\n",
      "Epoch : 62/100, Training Loss = 0.002433, Validation Loss = 0.002858\n",
      "Epoch : 63/100, Training Loss = 0.002404, Validation Loss = 0.002824\n",
      "Epoch : 64/100, Training Loss = 0.002435, Validation Loss = 0.002705\n",
      "Epoch : 65/100, Training Loss = 0.002324, Validation Loss = 0.002572\n",
      "Epoch : 66/100, Training Loss = 0.002317, Validation Loss = 0.002756\n",
      "Epoch : 67/100, Training Loss = 0.002392, Validation Loss = 0.002596\n",
      "Epoch : 68/100, Training Loss = 0.002285, Validation Loss = 0.002715\n",
      "Epoch : 69/100, Training Loss = 0.002329, Validation Loss = 0.002541\n",
      "Epoch : 70/100, Training Loss = 0.002235, Validation Loss = 0.002644\n",
      "Epoch : 71/100, Training Loss = 0.002270, Validation Loss = 0.002695\n",
      "Epoch : 72/100, Training Loss = 0.002253, Validation Loss = 0.002556\n",
      "Epoch : 73/100, Training Loss = 0.002275, Validation Loss = 0.002529\n",
      "Epoch : 74/100, Training Loss = 0.002199, Validation Loss = 0.002491\n",
      "Epoch : 75/100, Training Loss = 0.002204, Validation Loss = 0.002529\n",
      "Epoch : 76/100, Training Loss = 0.002194, Validation Loss = 0.002554\n",
      "Epoch : 77/100, Training Loss = 0.002212, Validation Loss = 0.002369\n",
      "Epoch : 78/100, Training Loss = 0.002187, Validation Loss = 0.002434\n",
      "Epoch : 79/100, Training Loss = 0.002126, Validation Loss = 0.002751\n",
      "Epoch : 80/100, Training Loss = 0.002182, Validation Loss = 0.002486\n",
      "Epoch : 81/100, Training Loss = 0.002146, Validation Loss = 0.002474\n",
      "Epoch : 82/100, Training Loss = 0.002195, Validation Loss = 0.002585\n",
      "Epoch : 83/100, Training Loss = 0.002160, Validation Loss = 0.002322\n",
      "Epoch : 84/100, Training Loss = 0.002146, Validation Loss = 0.002247\n",
      "Epoch : 85/100, Training Loss = 0.002145, Validation Loss = 0.002401\n",
      "Epoch : 86/100, Training Loss = 0.002124, Validation Loss = 0.002487\n",
      "Epoch : 87/100, Training Loss = 0.002065, Validation Loss = 0.002388\n",
      "Epoch : 88/100, Training Loss = 0.002115, Validation Loss = 0.002509\n",
      "Epoch : 89/100, Training Loss = 0.002109, Validation Loss = 0.002397\n",
      "Epoch : 90/100, Training Loss = 0.002068, Validation Loss = 0.002275\n",
      "Epoch : 91/100, Training Loss = 0.002082, Validation Loss = 0.002400\n",
      "Epoch : 92/100, Training Loss = 0.002041, Validation Loss = 0.002358\n",
      "Epoch : 93/100, Training Loss = 0.002028, Validation Loss = 0.002297\n",
      "Epoch : 94/100, Training Loss = 0.001996, Validation Loss = 0.002392\n",
      "Epoch : 95/100, Training Loss = 0.002056, Validation Loss = 0.002783\n",
      "Epoch : 96/100, Training Loss = 0.002092, Validation Loss = 0.002404\n",
      "Epoch : 97/100, Training Loss = 0.001990, Validation Loss = 0.002310\n",
      "Epoch : 98/100, Training Loss = 0.001994, Validation Loss = 0.002124\n",
      "Epoch : 99/100, Training Loss = 0.002032, Validation Loss = 0.002280\n",
      "Epoch : 100/100, Training Loss = 0.001965, Validation Loss = 0.002516\n",
      "[0.0026610386557877064, 0.018806150183081627, 0.006008965894579887, 0.03578958287835121, -0.05041636526584625, 0.04696540907025337, 0.0045943306758999825, 0.04114833101630211, 0.0668235644698143, -0.04136284813284874]\n",
      "Epochs: 100, Training Loss: 0.0019650233204616873, Validation Loss: 0.002515798045738422\n"
     ]
    }
   ],
   "source": [
    "# BASIC AUTOENCODER Execute training & validating\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "# number of hidden units in encoder hidden layer\n",
    "n_units = 50\n",
    "# number of hidden units in latent space\n",
    "latent_units = 10\n",
    "# Boolean for whether to use Early Stopping\n",
    "is_early_stopping = False\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "\n",
    "torch.manual_seed(5)\n",
    "basic_model = AutoEncoder(input_shape=n_features,\n",
    "                    n_units=n_units,\n",
    "                    latent_units=latent_units\n",
    "                   ).to(device)\n",
    "\n",
    "basic_trained = train_validate(model=basic_model,\n",
    "                            epochs=epochs,\n",
    "                            lr=lr,\n",
    "                            is_early_stopping=is_early_stopping, \n",
    "                            is_pca=is_pca,\n",
    "                            patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18dd5d37-5fac-4458-a85f-c78519f2d69d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Latent Layer Weights:\n",
      "[0.007588679436594248, -0.012624192982912064, 0.010443619452416897, 0.021243233233690262, -0.05041636526584625, 0.009892481379210949, 0.018681535497307777, 0.03931600600481033, 0.04897252097725868, -0.04136284813284874]\n",
      "Training...\n",
      "Epoch : 1/100, Training Loss = 0.093181, Validation Loss = 0.032523\n",
      "Average Sparsity = 0.0027955768164247274\n",
      "Epoch : 2/100, Training Loss = 0.030966, Validation Loss = 0.017787\n",
      "Average Sparsity = 0.0031247534789144993\n",
      "Epoch : 3/100, Training Loss = 0.017924, Validation Loss = 0.011352\n",
      "Average Sparsity = 0.003145939204841852\n",
      "Epoch : 4/100, Training Loss = 0.015235, Validation Loss = 0.010133\n",
      "Average Sparsity = 0.0031943167559802532\n",
      "Epoch : 5/100, Training Loss = 0.014435, Validation Loss = 0.009558\n",
      "Average Sparsity = 0.0032092994078993797\n",
      "Epoch : 6/100, Training Loss = 0.013819, Validation Loss = 0.009158\n",
      "Average Sparsity = 0.003149958560243249\n",
      "Epoch : 7/100, Training Loss = 0.013355, Validation Loss = 0.008689\n",
      "Average Sparsity = 0.0031161645893007517\n",
      "Epoch : 8/100, Training Loss = 0.012799, Validation Loss = 0.008346\n",
      "Average Sparsity = 0.003127245930954814\n",
      "Epoch : 9/100, Training Loss = 0.012328, Validation Loss = 0.007826\n",
      "Average Sparsity = 0.0031617237254977226\n",
      "Epoch : 10/100, Training Loss = 0.011861, Validation Loss = 0.007358\n",
      "Average Sparsity = 0.0031688320450484753\n",
      "Epoch : 11/100, Training Loss = 0.011547, Validation Loss = 0.007042\n",
      "Average Sparsity = 0.0031550342682749033\n",
      "Epoch : 12/100, Training Loss = 0.011245, Validation Loss = 0.006737\n",
      "Average Sparsity = 0.0031185687985271215\n",
      "Epoch : 13/100, Training Loss = 0.010965, Validation Loss = 0.006456\n",
      "Average Sparsity = 0.0031426798086613417\n",
      "Epoch : 14/100, Training Loss = 0.010742, Validation Loss = 0.006859\n",
      "Average Sparsity = 0.0031910662073642015\n",
      "Epoch : 15/100, Training Loss = 0.010475, Validation Loss = 0.006321\n",
      "Average Sparsity = 0.0031452011317014694\n",
      "Epoch : 16/100, Training Loss = 0.010182, Validation Loss = 0.005829\n",
      "Average Sparsity = 0.0031784584280103445\n",
      "Epoch : 17/100, Training Loss = 0.009964, Validation Loss = 0.005709\n",
      "Average Sparsity = 0.0031462146434932947\n",
      "Epoch : 18/100, Training Loss = 0.009742, Validation Loss = 0.005605\n",
      "Average Sparsity = 0.0031567306723445654\n",
      "Epoch : 19/100, Training Loss = 0.009442, Validation Loss = 0.005047\n",
      "Average Sparsity = 0.0032056502532213926\n",
      "Epoch : 20/100, Training Loss = 0.009209, Validation Loss = 0.004851\n",
      "Average Sparsity = 0.0031679742969572544\n",
      "Epoch : 21/100, Training Loss = 0.008994, Validation Loss = 0.004778\n",
      "Average Sparsity = 0.0031407682690769434\n",
      "Epoch : 22/100, Training Loss = 0.008782, Validation Loss = 0.004546\n",
      "Average Sparsity = 0.003160248976200819\n",
      "Epoch : 23/100, Training Loss = 0.008592, Validation Loss = 0.004346\n",
      "Average Sparsity = 0.003158612409606576\n",
      "Epoch : 24/100, Training Loss = 0.008399, Validation Loss = 0.004236\n",
      "Average Sparsity = 0.003151450539007783\n",
      "Epoch : 25/100, Training Loss = 0.008321, Validation Loss = 0.004055\n",
      "Average Sparsity = 0.003169786650687456\n",
      "Epoch : 26/100, Training Loss = 0.008109, Validation Loss = 0.003776\n",
      "Average Sparsity = 0.0031675910577178\n",
      "Epoch : 27/100, Training Loss = 0.008004, Validation Loss = 0.004023\n",
      "Average Sparsity = 0.0031802032608538866\n",
      "Epoch : 28/100, Training Loss = 0.007955, Validation Loss = 0.003841\n",
      "Average Sparsity = 0.003189654089510441\n",
      "Epoch : 29/100, Training Loss = 0.007788, Validation Loss = 0.003653\n",
      "Average Sparsity = 0.0031819611322134733\n",
      "Epoch : 30/100, Training Loss = 0.007733, Validation Loss = 0.003387\n",
      "Average Sparsity = 0.0031039726454764605\n",
      "Epoch : 31/100, Training Loss = 0.007703, Validation Loss = 0.003641\n",
      "Average Sparsity = 0.0032052972819656134\n",
      "Epoch : 32/100, Training Loss = 0.007633, Validation Loss = 0.003528\n",
      "Average Sparsity = 0.0031760793644934893\n",
      "Epoch : 33/100, Training Loss = 0.007611, Validation Loss = 0.003603\n",
      "Average Sparsity = 0.003225152613595128\n",
      "Epoch : 34/100, Training Loss = 0.007514, Validation Loss = 0.003360\n",
      "Average Sparsity = 0.003206283086910844\n",
      "Epoch : 35/100, Training Loss = 0.007483, Validation Loss = 0.003798\n",
      "Average Sparsity = 0.003196755889803171\n",
      "Epoch : 36/100, Training Loss = 0.007460, Validation Loss = 0.003331\n",
      "Average Sparsity = 0.00319305039010942\n",
      "Epoch : 37/100, Training Loss = 0.007410, Validation Loss = 0.003216\n",
      "Average Sparsity = 0.0032148368190973997\n",
      "Epoch : 38/100, Training Loss = 0.007290, Validation Loss = 0.003328\n",
      "Average Sparsity = 0.0031842913012951612\n",
      "Epoch : 39/100, Training Loss = 0.007255, Validation Loss = 0.003280\n",
      "Average Sparsity = 0.0031840307638049126\n",
      "Epoch : 40/100, Training Loss = 0.007222, Validation Loss = 0.002977\n",
      "Average Sparsity = 0.0031763010192662477\n",
      "Epoch : 41/100, Training Loss = 0.007240, Validation Loss = 0.003172\n",
      "Average Sparsity = 0.003141767345368862\n",
      "Epoch : 42/100, Training Loss = 0.007189, Validation Loss = 0.003101\n",
      "Average Sparsity = 0.003223934443667531\n",
      "Epoch : 43/100, Training Loss = 0.007161, Validation Loss = 0.003084\n",
      "Average Sparsity = 0.00316596613265574\n",
      "Epoch : 44/100, Training Loss = 0.007110, Validation Loss = 0.003041\n",
      "Average Sparsity = 0.0031871735118329525\n",
      "Epoch : 45/100, Training Loss = 0.007073, Validation Loss = 0.003271\n",
      "Average Sparsity = 0.0031910527031868696\n",
      "Epoch : 46/100, Training Loss = 0.007054, Validation Loss = 0.003114\n",
      "Average Sparsity = 0.0031354501843452454\n",
      "Epoch : 47/100, Training Loss = 0.007056, Validation Loss = 0.003065\n",
      "Average Sparsity = 0.0032259237486869097\n",
      "Epoch : 48/100, Training Loss = 0.007023, Validation Loss = 0.003025\n",
      "Average Sparsity = 0.003190629417076707\n",
      "Epoch : 49/100, Training Loss = 0.006974, Validation Loss = 0.003036\n",
      "Average Sparsity = 0.003172948956489563\n",
      "Epoch : 50/100, Training Loss = 0.006981, Validation Loss = 0.002778\n",
      "Average Sparsity = 0.0032022749073803425\n",
      "Epoch : 51/100, Training Loss = 0.006941, Validation Loss = 0.002886\n",
      "Average Sparsity = 0.0032012935262173414\n",
      "Epoch : 52/100, Training Loss = 0.006904, Validation Loss = 0.003398\n",
      "Average Sparsity = 0.0032105029094964266\n",
      "Epoch : 53/100, Training Loss = 0.006958, Validation Loss = 0.003095\n",
      "Average Sparsity = 0.0031941707711666822\n",
      "Epoch : 54/100, Training Loss = 0.006825, Validation Loss = 0.002920\n",
      "Average Sparsity = 0.003206101944670081\n",
      "Epoch : 55/100, Training Loss = 0.006872, Validation Loss = 0.002865\n",
      "Average Sparsity = 0.0031913069542497396\n",
      "Epoch : 56/100, Training Loss = 0.006783, Validation Loss = 0.002636\n",
      "Average Sparsity = 0.003160374704748392\n",
      "Epoch : 57/100, Training Loss = 0.006804, Validation Loss = 0.002917\n",
      "Average Sparsity = 0.0031926303636282682\n",
      "Epoch : 58/100, Training Loss = 0.006771, Validation Loss = 0.002984\n",
      "Average Sparsity = 0.0032065040431916714\n",
      "Epoch : 59/100, Training Loss = 0.006724, Validation Loss = 0.002932\n",
      "Average Sparsity = 0.003144365269690752\n",
      "Epoch : 60/100, Training Loss = 0.006857, Validation Loss = 0.003087\n",
      "Average Sparsity = 0.0031849548686295748\n",
      "Epoch : 61/100, Training Loss = 0.006680, Validation Loss = 0.002741\n",
      "Average Sparsity = 0.0031684974674135447\n",
      "Epoch : 62/100, Training Loss = 0.006709, Validation Loss = 0.002936\n",
      "Average Sparsity = 0.0031576273031532764\n",
      "Epoch : 63/100, Training Loss = 0.006744, Validation Loss = 0.002709\n",
      "Average Sparsity = 0.0031551013235002756\n",
      "Epoch : 64/100, Training Loss = 0.006689, Validation Loss = 0.003057\n",
      "Average Sparsity = 0.0031403801403939724\n",
      "Epoch : 65/100, Training Loss = 0.006669, Validation Loss = 0.002808\n",
      "Average Sparsity = 0.003159558866173029\n",
      "Epoch : 66/100, Training Loss = 0.006638, Validation Loss = 0.002897\n",
      "Average Sparsity = 0.0031418907456099987\n",
      "Epoch : 67/100, Training Loss = 0.006666, Validation Loss = 0.002701\n",
      "Average Sparsity = 0.0032164789736270905\n",
      "Epoch : 68/100, Training Loss = 0.006580, Validation Loss = 0.002835\n",
      "Average Sparsity = 0.0032045776024460793\n",
      "Epoch : 69/100, Training Loss = 0.006629, Validation Loss = 0.002851\n",
      "Average Sparsity = 0.0031674979254603386\n",
      "Epoch : 70/100, Training Loss = 0.006564, Validation Loss = 0.002721\n",
      "Average Sparsity = 0.003180163912475109\n",
      "Epoch : 71/100, Training Loss = 0.006556, Validation Loss = 0.002543\n",
      "Average Sparsity = 0.0031937335152179003\n",
      "Epoch : 72/100, Training Loss = 0.006602, Validation Loss = 0.002565\n",
      "Average Sparsity = 0.003177147824317217\n",
      "Epoch : 73/100, Training Loss = 0.006494, Validation Loss = 0.002749\n",
      "Average Sparsity = 0.003175112884491682\n",
      "Epoch : 74/100, Training Loss = 0.006512, Validation Loss = 0.002687\n",
      "Average Sparsity = 0.003104891162365675\n",
      "Epoch : 75/100, Training Loss = 0.006513, Validation Loss = 0.003363\n",
      "Average Sparsity = 0.0031980499625205994\n",
      "Epoch : 76/100, Training Loss = 0.006609, Validation Loss = 0.002660\n",
      "Average Sparsity = 0.003197923768311739\n",
      "Epoch : 77/100, Training Loss = 0.006500, Validation Loss = 0.002637\n",
      "Average Sparsity = 0.00320612289942801\n",
      "Epoch : 78/100, Training Loss = 0.006441, Validation Loss = 0.002498\n",
      "Average Sparsity = 0.0032343531493097544\n",
      "Epoch : 79/100, Training Loss = 0.006481, Validation Loss = 0.003377\n",
      "Average Sparsity = 0.0031929679680615664\n",
      "Epoch : 80/100, Training Loss = 0.006469, Validation Loss = 0.002711\n",
      "Average Sparsity = 0.0031520486809313297\n",
      "Epoch : 81/100, Training Loss = 0.006423, Validation Loss = 0.002414\n",
      "Average Sparsity = 0.003195221535861492\n",
      "Epoch : 82/100, Training Loss = 0.006434, Validation Loss = 0.002589\n",
      "Average Sparsity = 0.003165920963510871\n",
      "Epoch : 83/100, Training Loss = 0.006492, Validation Loss = 0.002419\n",
      "Average Sparsity = 0.0032345354557037354\n",
      "Epoch : 84/100, Training Loss = 0.006428, Validation Loss = 0.002602\n",
      "Average Sparsity = 0.0031402152962982655\n",
      "Epoch : 85/100, Training Loss = 0.006425, Validation Loss = 0.002448\n",
      "Average Sparsity = 0.00320682511664927\n",
      "Epoch : 86/100, Training Loss = 0.006428, Validation Loss = 0.002418\n",
      "Average Sparsity = 0.0031055384315550327\n",
      "Epoch : 87/100, Training Loss = 0.006352, Validation Loss = 0.002747\n",
      "Average Sparsity = 0.0031766833271831274\n",
      "Epoch : 88/100, Training Loss = 0.006361, Validation Loss = 0.002657\n",
      "Average Sparsity = 0.0031920094043016434\n",
      "Epoch : 89/100, Training Loss = 0.006400, Validation Loss = 0.002476\n",
      "Average Sparsity = 0.0031701521947979927\n",
      "Epoch : 90/100, Training Loss = 0.006386, Validation Loss = 0.002695\n",
      "Average Sparsity = 0.0032344332430511713\n",
      "Epoch : 91/100, Training Loss = 0.006393, Validation Loss = 0.002493\n",
      "Average Sparsity = 0.0031754975207149982\n",
      "Epoch : 92/100, Training Loss = 0.006384, Validation Loss = 0.002644\n",
      "Average Sparsity = 0.00318352528847754\n",
      "Epoch : 93/100, Training Loss = 0.006378, Validation Loss = 0.002545\n",
      "Average Sparsity = 0.003210576483979821\n",
      "Epoch : 94/100, Training Loss = 0.006283, Validation Loss = 0.002336\n",
      "Average Sparsity = 0.003197048557922244\n",
      "Epoch : 95/100, Training Loss = 0.006285, Validation Loss = 0.002514\n",
      "Average Sparsity = 0.00314270774833858\n",
      "Epoch : 96/100, Training Loss = 0.006376, Validation Loss = 0.002339\n",
      "Average Sparsity = 0.003173501929268241\n",
      "Epoch : 97/100, Training Loss = 0.006311, Validation Loss = 0.002672\n",
      "Average Sparsity = 0.0031799643766134977\n",
      "Epoch : 98/100, Training Loss = 0.006349, Validation Loss = 0.002312\n",
      "Average Sparsity = 0.0031760497950017452\n",
      "Epoch : 99/100, Training Loss = 0.006260, Validation Loss = 0.002463\n",
      "Average Sparsity = 0.0031746062450110912\n",
      "Epoch : 100/100, Training Loss = 0.006299, Validation Loss = 0.002451\n",
      "Average Sparsity = 0.0031742153223603964\n",
      "[0.0026845107786357403, 0.011647467501461506, 0.006008450873196125, 0.03188667818903923, -0.05041636526584625, 0.05312928929924965, 0.006130595225840807, 0.03967572748661041, 0.0630728006362915, -0.04136284813284874]\n",
      "Epochs: 100, Training Loss: 0.006298518958650359, Validation Loss: 0.0024505593873401905\n"
     ]
    }
   ],
   "source": [
    "# SPARSE AUTOENCODER Execute training & validating\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "n_units = 50\n",
    "latent_units = 10\n",
    "is_early_stopping = False\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "\n",
    "is_sparse = True\n",
    "beta = 1e-4\n",
    "rho = 0.005\n",
    "\n",
    "torch.manual_seed(5)\n",
    "sparse_model = AutoEncoder(input_shape=n_features,\n",
    "                    n_units=n_units,\n",
    "                    latent_units=latent_units\n",
    "                   ).to(device)\n",
    "\n",
    "sparse_trained = train_validate(model=sparse_model,\n",
    "                                epochs=epochs,\n",
    "                                lr=lr,\n",
    "                                is_early_stopping=is_early_stopping, \n",
    "                                is_pca=is_pca,\n",
    "                                is_sparse=is_sparse,\n",
    "                                patience=patience,\n",
    "                                beta=beta,\n",
    "                                rho=rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa486c-6a31-4369-bca0-8818b51992ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANDBOX\n",
    "\n",
    "# lr = 1e-3\n",
    "# epochs = 200\n",
    "# n_units = 50\n",
    "# latent_units = 20\n",
    "# is_early_stopping = True\n",
    "# patience = 5\n",
    "# is_sparse = True\n",
    "# beta = 2\n",
    "# rho = 0.1\n",
    "\n",
    "# model = AutoEncoder(input_shape=n_features,\n",
    "#                     n_units=n_units,\n",
    "#                     latent_units=latent_units\n",
    "#                    ).to(device)\n",
    "\n",
    "# b_hat = torch.tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0016, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "# b_hat = b_hat.to(device)\n",
    "# layer_enc = list(model.bottleneck.children())[0]\n",
    "# relu = list(model.bottleneck.children())[1]\n",
    "# layer_enc(b_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6ec0b-3e93-481f-9ed3-61ce51d04598",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH =r'C:\\Users\\evere\\Documents\\research\\bigan\\models'\n",
    "torch.save(basic_trained.model.state_dict(), os.path.join(PATH, 'basic_batch32_2.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834a8e2-4918-4a96-abfe-f4860ead4a29",
   "metadata": {},
   "source": [
    "# Helmholtz-Latent Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ae87a-8aa8-4f8e-8832-53b15a378428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN IF LOADING EXISTING MODEL\n",
    "# n_units = 50\n",
    "# latent_units = 4\n",
    "\n",
    "# basic_pretrained = AutoEncoder(input_shape=n_features,\n",
    "#                     n_units=n_units,\n",
    "#                     latent_units=latent_units\n",
    "#                    ).to(device)\n",
    "\n",
    "# PATH = r'C:\\Users\\evere\\Documents\\research\\bigan\\models\\basic_batch32_2.pth'\n",
    "# basic_pretrained.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dd8f6fc-ac70-4663-b8d6-7699cd49e223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0005, device='cuda:0'),\n",
       " tensor(0.0209, device='cuda:0'),\n",
       " tensor(0.0056, device='cuda:0'),\n",
       " tensor(0.0396, device='cuda:0'),\n",
       " tensor(-0.0504, device='cuda:0'),\n",
       " tensor(0.0519, device='cuda:0'),\n",
       " tensor(0.0048, device='cuda:0'),\n",
       " tensor(0.0393, device='cuda:0'),\n",
       " tensor(0.0662, device='cuda:0'),\n",
       " tensor(-0.0502, device='cuda:0')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get average BASIC latent layer weights\n",
    "avg_basic_weights = []\n",
    "for neuron in basic_trained.model.state_dict()['bottleneck.0.weight']:\n",
    "    avg = torch.mean(neuron)\n",
    "    avg_basic_weights.append(avg)\n",
    "avg_basic_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c76e6829-0751-445d-88c6-fa837f45bb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0027, device='cuda:0'),\n",
       " tensor(0.0116, device='cuda:0'),\n",
       " tensor(0.0060, device='cuda:0'),\n",
       " tensor(0.0319, device='cuda:0'),\n",
       " tensor(-0.0504, device='cuda:0'),\n",
       " tensor(0.0531, device='cuda:0'),\n",
       " tensor(0.0061, device='cuda:0'),\n",
       " tensor(0.0397, device='cuda:0'),\n",
       " tensor(0.0631, device='cuda:0'),\n",
       " tensor(-0.0414, device='cuda:0')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get average SPARSE latent layer weights\n",
    "avg_sparse_weights = []\n",
    "for neuron in sparse_trained.model.state_dict()['bottleneck.0.weight']:\n",
    "    avg = torch.mean(neuron)\n",
    "    avg_sparse_weights.append(avg)\n",
    "avg_sparse_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a0874-95fc-4c4d-8c7e-b1f2dec888e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_activations(\n",
    "        activations: DefaultDict,\n",
    "        name: str,\n",
    "        module: nn.Module,\n",
    "        inp: Tuple,\n",
    "        out: torch.Tensor\n",
    ") -> None:\n",
    "    \"\"\"PyTorch Forward hook to save outputs at each forward\n",
    "    pass. Mutates specified dict objects with each fwd pass.\n",
    "    \"\"\"\n",
    "    activations[name].append(out.detach().cpu())\n",
    "    \n",
    "\n",
    "def register_activation_hooks(\n",
    "        model: nn.Module,\n",
    "        layers_to_save: List[str]\n",
    ") -> DefaultDict[List, torch.Tensor]:\n",
    "    \"\"\"Registers forward hooks in specified layers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        PyTorch model\n",
    "    layers_to_save:\n",
    "        Module names within ``model`` whose activations we want to save.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    activations_dict:\n",
    "        dict of lists containing activations of specified layers in\n",
    "        ``layers_to_save``.\n",
    "    \"\"\"\n",
    "    activations_dict = collections.defaultdict(list)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name in layers_to_save:\n",
    "            module.register_forward_hook(\n",
    "                partial(save_activations, activations_dict, name)\n",
    "            )\n",
    "    return activations_dict\n",
    "\n",
    "\n",
    "# Save activations per layer per sample\n",
    "def get_activations(model):\n",
    "    # Enter which layers to retrieve activations from\n",
    "    to_save = ['bottleneck.0']\n",
    "\n",
    "    # register fwd hooks in specified layers\n",
    "    saved_activations = register_activation_hooks(model, layers_to_save=to_save)\n",
    "    activations_with_params = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Evaluate one sample at a time\n",
    "        for i, sample in enumerate(test_loader_stoch, 0):\n",
    "            # Remove params, keep data\n",
    "            params = sample[1]\n",
    "            sample = sample[0]\n",
    "            # move to device\n",
    "            sample = sample.to(device)\n",
    "            # bottleneck layer is returned by default, but we do not use it here,\n",
    "            # opting instead to use forward hooks.\n",
    "            bottleneck, reconstruction = model(sample)\n",
    "            \n",
    "            # keep track of which activations correspond with which parameters\n",
    "            pair = [saved_activations['bottleneck.0'][i], params]\n",
    "            activations_with_params.append(pair)\n",
    "            \n",
    "    return activations_with_params\n",
    "\n",
    "\n",
    "def plot_correlations(activations_with_params, model_name):\n",
    "    SAVE_PATH = f'visualizations/param_activation_corr/{model_name}'\n",
    "    \n",
    "    param0 = np.empty([1500])\n",
    "    param1 = np.empty([1500])\n",
    "    param2 = np.empty([1500])\n",
    "    param3 = np.empty([1500])\n",
    "    \n",
    "    activ0 = np.empty([1500])\n",
    "    activ1 = np.empty([1500])\n",
    "    activ2 = np.empty([1500])\n",
    "    activ3 = np.empty([1500])\n",
    "\n",
    "    for i, pair in enumerate(activations_with_params):\n",
    "        activations = pair[0]\n",
    "        params = pair[1]\n",
    "        \n",
    "        param0[i] = params[0][0]\n",
    "        param1[i] = params[0][1]\n",
    "        param2[i] = params[0][2]\n",
    "        param3[i] = params[0][3]\n",
    "        \n",
    "        activ0[i] = params[0][0]\n",
    "        activ1[i] = params[0][1]\n",
    "        activ2[i] = params[0][2]\n",
    "        activ3[i] = params[0][3]\n",
    "        \n",
    "    print(f'{len(activations_with_params)} samples in test set.')\n",
    "        \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 0, Activations with Neuron 0\")\n",
    "    plt.scatter(param0, activ0)\n",
    "    path = SAVE_PATH + '/param0activ0.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 0, Activations with Neuron 0 (Reduced Axis Range)\")\n",
    "    plt.scatter(param0, activ0)\n",
    "    plt.axis([1000, 1500, 1000, 1500])\n",
    "    path = SAVE_PATH + '/param0activ0_reduced.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 0, Activations with Neuron 1\")\n",
    "    plt.scatter(param0, activ1)\n",
    "    path = SAVE_PATH + '/param0activ1.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 0, Activations with Neuron 2\")\n",
    "    plt.scatter(param0, activ2)\n",
    "    path = SAVE_PATH + '/param0activ2.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 0, Activations with Neuron 3\")\n",
    "    plt.scatter(param0, activ3)\n",
    "    path = SAVE_PATH + '/param0activ3.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 1, Activations with Neuron 0\")\n",
    "    plt.scatter(param1, activ0)\n",
    "    path = SAVE_PATH + '/param1activ0.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 1, Activations with Neuron 1\")\n",
    "    plt.scatter(param1, activ1)\n",
    "    path = SAVE_PATH + '/param1activ1.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 1, Activations with Neuron 2\")\n",
    "    plt.scatter(param1, activ2)\n",
    "    path = SAVE_PATH + '/param1activ2.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 1, Activations with Neuron 3\")\n",
    "    plt.scatter(param1, activ3)\n",
    "    path = SAVE_PATH + '/param1activ3.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 2, Activations with Neuron 0\")\n",
    "    plt.scatter(param2, activ0)\n",
    "    path = SAVE_PATH + '/param2activ0.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 2, Activations with Neuron 1\")\n",
    "    plt.scatter(param2, activ1)\n",
    "    path = SAVE_PATH + '/param2activ1.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 2, Activations with Neuron 2\")\n",
    "    plt.scatter(param2, activ2)\n",
    "    path = SAVE_PATH + '/param2activ2.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 2, Activations with Neuron 3\")\n",
    "    plt.scatter(param2, activ3)\n",
    "    path = SAVE_PATH + '/param2activ3.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 3, Activations with Neuron 0\")\n",
    "    plt.scatter(param3, activ0)\n",
    "    path = SAVE_PATH + '/param3activ0.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 3, Activations with Neuron 1\")\n",
    "    plt.scatter(param3, activ1)\n",
    "    path = SAVE_PATH + '/param3activ1.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 3, Activations with Neuron 2\")\n",
    "    plt.scatter(param3, activ2)\n",
    "    path = SAVE_PATH + '/param3activ2.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Helmholtz Parameter 3, Activations with Neuron 3\")\n",
    "    plt.scatter(param3, activ3)\n",
    "    path = SAVE_PATH + '/param3activ3.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf51a6-191f-4ee6-9d79-07afb1d3010d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activations_with_params = get_activations(sparse_trained.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270c086-909c-4ca7-8094-7f498525fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'basic_batch32_2'\n",
    "plot_correlations(activations_with_params, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ff135-bccb-4e15-938f-98ceaf60be72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b25a7fcd-6d6b-4233-a89d-a446f57a2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, n_features):\n",
    "    # Decoupled into three lists due to issue with placing torch tensors into multidimensional lists\n",
    "    batches = []\n",
    "    recons = []\n",
    "    test_losses = []\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Prepare model for evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader, 0):\n",
    "            # remove params, keep data\n",
    "            batch = batch[0]\n",
    "            # move data to device\n",
    "            batch = batch.to(device)\n",
    "            bottleneck, reconstructions = model(batch)\n",
    "            # Reconstruction loss\n",
    "            test_loss = criterion(reconstructions, batch)\n",
    "            # Store samples, predictions, and loss for visualization purposes\n",
    "            batches.append(batch)\n",
    "            recons.append(reconstructions)\n",
    "            test_losses.append(test_loss.item())\n",
    "            print(f'Batch {i}: {test_loss.item()}')\n",
    "\n",
    "    avg_test_loss = np.average(test_losses)\n",
    "    print(f\"Average Test Reconstruction Loss: {avg_test_loss}\")\n",
    "    \n",
    "    return batches, recons, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac96cd0-4595-40f2-9b59-37ffc74b485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC MODEL TEST\n",
    "batches, recons, test_losses = test(basic_trained.model, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e14ca1c5-3d5e-4f1f-874b-f586ec9cc44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: 0.009051457978785038\n",
      "Batch 1: 0.006962968036532402\n",
      "Batch 2: 0.008486664853990078\n",
      "Batch 3: 0.006829945370554924\n",
      "Batch 4: 0.006951460614800453\n",
      "Batch 5: 0.010006973519921303\n",
      "Batch 6: 0.004857387859374285\n",
      "Batch 7: 0.007002027239650488\n",
      "Batch 8: 0.005335994530469179\n",
      "Batch 9: 0.009473899379372597\n",
      "Batch 10: 0.007658135145902634\n",
      "Batch 11: 0.006438864395022392\n",
      "Batch 12: 0.008227875456213951\n",
      "Batch 13: 0.0070784008130431175\n",
      "Batch 14: 0.006576946005225182\n",
      "Batch 15: 0.008005345240235329\n",
      "Batch 16: 0.005998560693114996\n",
      "Batch 17: 0.005109238438308239\n",
      "Batch 18: 0.006268978118896484\n",
      "Batch 19: 0.009302851743996143\n",
      "Batch 20: 0.006902267690747976\n",
      "Batch 21: 0.007767305243760347\n",
      "Batch 22: 0.011868637055158615\n",
      "Batch 23: 0.006220769137144089\n",
      "Batch 24: 0.006589134223759174\n",
      "Batch 25: 0.008141114376485348\n",
      "Batch 26: 0.005145126022398472\n",
      "Batch 27: 0.007765709422528744\n",
      "Batch 28: 0.004028357099741697\n",
      "Batch 29: 0.006486959755420685\n",
      "Batch 30: 0.006531390827149153\n",
      "Batch 31: 0.0063080089166760445\n",
      "Batch 32: 0.007550864014774561\n",
      "Batch 33: 0.004589482676237822\n",
      "Batch 34: 0.007705891039222479\n",
      "Batch 35: 0.007474071811884642\n",
      "Batch 36: 0.0057646650820970535\n",
      "Batch 37: 0.006510840728878975\n",
      "Batch 38: 0.007971649058163166\n",
      "Batch 39: 0.00915690790861845\n",
      "Batch 40: 0.006769103463739157\n",
      "Batch 41: 0.006456784904003143\n",
      "Batch 42: 0.010522722266614437\n",
      "Batch 43: 0.008898785337805748\n",
      "Batch 44: 0.0061863181181252\n",
      "Batch 45: 0.008013477548956871\n",
      "Batch 46: 0.005159891210496426\n",
      "Average Test Reconstruction Loss: 0.007193834263276547\n"
     ]
    }
   ],
   "source": [
    "# SPARSE MODEL TEST\n",
    "batches, recons, test_losses = test(sparse_trained.model, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0797b-b169-4534-a8cb-bf679aad6a30",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "411133d1-7a8f-4acf-9c70-3fb090cef252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAAHRCAYAAADnk4nDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc6klEQVR4nO3debgtV1kn4N+X6SYQMkCYggGEADK00NIoaUFRwEAQpxZEEAQBiTa2ihrUBzRCQGi1FUREWxkicwPNlNhgt0maQRFaJRGCSCADJIEEcm/mEHJX/1F1zM717HO+O+UOed/n2c89u1bVqrVrV9WvhlX71hgjAMDa9tnVDQCAPYHABIAGgQkADQITABoEJgA0CEwAaBCYANAgMHdDVXVuVT1qV7djT1ZVo6qO3tXt2BtU1ZVVdY9d3Q7Y1QRm0xxi18w7j8uq6pSqOqo57d3nHfh+O7udq8z7AVX1gaq6tKpW/ZWKqjqyqr64xbB7VdW1VfXGhWFPmT//yuvq+XM9uNmWUVVXzdNeWlVvqarDmtM+Yss27ihVdWJVXT+3a2NVfbSqjtkZ89peVfX6qjppJ9Z/elU9a3HYGOPgMcbnd8K89qgDw6raUFWvrarLq+riqnreOuM/uarOm9f5d1fVbbt1VdXjq+qf5nXyo1V1vy3Kf3GebtNcz4aFstPnbXdlO/3nhbIDquod87IfVfWI7V0utyQCc+s8foxxcJI7J/lykj/cxe3puD7J25M8c41xjkvyv7YY9kdJPr44YIzxpnnnefC8HH42yeeT/P1WtOeB87T3SHJ4khO3Ytqd6W1zu45IclqS/7GL27NNdsVB2S3IiUnuleRuSb4nyQlV9ZjVRqyq+yf5kyRPTXLHJFcneXWnrqq6V5I3JTk+yWFJ3pfkvSvfbVUdm+RXkzwyyd0zbUu/tUUTnruwrd5ni7IPJ/mJJBdvzYcnyRjDq/FKcm6SRy28Py7JZxfePy7JPyS5PMkFSU5cKDs/yUhy5fw6Zh7+7CRnJ7kiyaeTfNvCvH45yZlJNiV5W5IDt7P9R09f96pl70ryIwvvn5QpZE9M8sY16jwtyW9uRRtGkqMX3v9skg8uvH/GwvL4fJLnzMNvneSaJJsXluGRSfZN8utJzpmn+X9JjlqY1/FJ/iXJZZkOAGpJu27yOZPcb57+9vP7Q5P8eZKLknwpyUlJ9l0Yf9n3eN8kpyfZmORTSX5gYZrXz206ZZ7uY0nuOZdVkt9P8pX5+z8zyQOS/HSmA6Cvz8vgfQvry/Pn8a5Lst8qy/r1SU5aeP+DSf4x0/p6TpLHJHlJkhuSXDvX/6otv7d5WZyc5JIk5yV5QZJ95rKnZ9oZ/+68zL+Q5LHdbWph+IYkf5Dkwvn1B0k2zGVHJHn/vEy/luRDC/N//vz9XJHkn5M8cgfvA76U5PsW3r84yVuXjPvSJG9eeH/P+Xu7zXp1JXluklMWyvbJtP4/cn7/5iQvXSh/ZJKLF96fnuRZjc/zxSSP2JHLaG9/7fIG7CmvxY07ya2SvCHJyQvlj0jy7+aV+1sznYH+0Fx293mns9/C+E+YN5qHZNpBHp3kbgvz+rtMoXDbTDvj45e0667zzuOu67R/1cBMsn+SSxc25EOSfDbJUVkjMDMdGd+Q5Ju3Yhku7ngPT/LBJC9aKH/cvGOpJN+d6ah8JXwekeSLW9T3K0nOSnKfeZoHJrndwrzen+kI/a6ZdvCPWdKuf/2cSQ5I8rJ5mew3D3t3prOFWye5w/zdrIT5qt/jvFw/lynQD0jyvZl25PeZp3t9ph3+t2cKuDflxh3msZnC/7C5zvsmufPCdCdt0f5zM4XfUUkO2nJZbzndPM9NSR6daX29S5JvmctOzxY72y2+t5OTvCfJbTKt159N8sy57OmZAv3ZmQ5mfiZT4C07UDk3qwfmi5L87bysb5/ko0lePJf9dpLXzMt3/yQPn5fRfTIdqB65sM3dc8l8fzXTNrPqa8k0h8/L4Y4Lw340yVlLxn9PkudvMezKJA9er64kP5fk1IWyfTMdxPz8/P6TSX5sofyIub6Vdf/0TOv7pUk+kiWhGIG51S+XZLfOu6tqY6aj8kcn+Z2VgjHG6WOMs8YYm8cYZyZ5S6ad/jLPSvJfxxgfH5PPjTHOWyh/5RjjwjHG1zJdknnQapWMMc4fYxw2xjh/Gz/TdyX55Bjjivn9i5P8+RjjgnWme1qSD40xvrCV8/v7eRleminI/mSlYIxxyhjjnHl5nJEpUB++Rl3PSvKCMcY/z9N8cozx1YXyl40xNs7L5rQsWYazJ87tuibTDv9HxxjfqKo7Jnlskl8YY1w1xvhKprO/Jy20YbXv8aFJDp7b8PUxxl9nCvAfX5jnu8YYfzfG+EamwFxp3/WZAulbMoXN2WOMi9ZoezKtLxeMMa5ZZ7xkujz/2jHGX83r65fGGJ9Zb6Kq2jfJjyX5tTHGFWOMc5P8XqbLjivOG2P89zHGDZkOKu+c6ZLk1nhKpgOpr4wxLsl0uXFlHtfPdd5tjHH9GONDY4yR6eBtQ5L7VdX+Y4xzxxjnrFb5GONl8zaz6mtJmw6e/920MGxTpu9p2fibthi2Mv56df1Vku+e79sfkBsPum61pO6Vv1emf36my7R3SfKnSd5XVfdc0k62gsDcOj80b1AbMl02OaOq7pQkVfUdVXVaVV1SVZsyXQ48Yo26jsp0KWyZxfsLV+fGjWxHOy7JqUlSVQ9K8qhMgbCep2XaIW6tb5uX4YFJ/jjJh6rqwHn+j62qv62qr83hdVxuvmX49rldd0zyT5nOBJIbzxYvmjsEbcwU8ndYpw1HJrlgjLF5Ydh5mXZia7ZvDtdXZbpk++Wq+tOqOmSNtifT2VXXesttmSMy7bgXD+yWfqYxxtXzn1u77h65yjyOnP/+nUxn7h+sqs9X1a/O8/pckl/IdLXgK1X11qo6MjvOlfO/i9/DIZmuGiwbf8vvbGX8NeuaD15+MtM6cFGm5f7pTGeEq9W98vfK9B+bD2iuG2O8IdNZ5nHrfD4aBOY2GGPcMMZ4V6aj2ofNg9+c5L2Z7qEdmumyUa1Msko1F2S6/LirHZfpPloyXfa8e5Lzq+riTPdR/1NV3aRTT1V9Z6Yd2Du2daZjjOuT/FmSb07ygLmX3zsz3f+64xxep+ZmXoZjjEuTPCfJiVV153ke1yU5YuEs5JAxxv3XacOFSY6qqsVt7K6ZLt922vHKMcaDk9w/yb0zXX5OVl8Oqw2/OjeekSTJnRb+Xmu5Las/ma4KXJ/pIGJF+zNthQtXmceFSTIHwS+NMe6R5PFJnldVj5zL3jzGeNg87Ujy8tUqr6pf36K3901eq00zxrgsU3g9cGHwAzPdm17NpxbHremxnA2Z+j2sW9cY4x1jjAeMMW6X5Dfnz7TSCe9Tq0z75S2urtyk+blxO2I7CMxtUJMfzHQv4ux58G2SfG2McW1VfXuSJy9MckmmDiuLz7L9WZJfrqoHz/UdXVWLO4kd2dYDM50ZpKoOXOmCXlXfnKkzxcrluD/NtCN90Px6TaYwPXaLan8yyTsXLuOuzOvpVXVus137Zurkc02mDj4HZNqhXJLkG1X12CTftzDJl5PcrqoOXRj2Z0leXNMjMFVV31pVt+vMfy3z8vhAkhPmS6EfTPJ7VXVIVe1TVfesqpXL7cu+x48luSpT78f95+77j0/y1vXmX1UPma9Y7D/XcW2mg7OV5dB5JvIfkzy5qvade18u3h748yTPqKpHzp/nLlX1LevVP19mfXuSl1TVbebP+bwkb1xt/Kb953Vy5bVfptsZL6iq21fVEUl+Y2UeVfX98zKuTLdGbkhyQ1Xdp6q+d163r820Xt2w2gzHGC8dC729t3yt0daT53YdPi+vZ2e6N7yaNyV5fFU9vKpunem+7LsWtpk165rXp32r6vaZrmi8b2E7PTnJM6vqflV1eKaOV6+fpzusqo5dWZZV9ZRMt10+sFD3hpWrOkkOmMcVqB1jN7iRuie8MnVQuCbT5ZArMl22e8pC+Y9munR0RaZ7Va/KTXtevihTGGxM8tB52PGZevNdOdf37xfmtdgj98Qs73xz13n6VTv95MYOR4uvc+ey52buCblk2n8z30yXUjdmlR6ISV6Y5E1r1DcyBcCVmXZ2H09y7EL5f860w96Y5C8yhctiz87XJvnqXL7SS/YFmXpjXjHX900L81raS7TxOb9jbusdMvUM/eNMl8Q2ZeoN/aSFcZd9j/dPcsY8zaeT/PCy9mShU1OmXo9nzvVdmmnne/Bcdq9MYbgxybtXW1/mYf8h05nIFfOyfMsW8/vheR5XZLrEeew8/JhMHXkuy3Rf9CbLMtNB4hszrcsXZAqzm/SSXeU7P3rJcj83/3bdPCnTOvbKTGdhF81/HzhP84vzdFfN38cL5+Hfmqkz1hWZOlO9P3MHoB24D9iQaR28PNN6+rwtyq9M8vCF90/O1EP+qkydgG67FXV9eOGz/EmSW29R/rx5usuTvC439iK+fabt4Ip5HfnbJI9uLPe778hltbe+al6A3AJV1amZAvPUHVTfBzP15Dt73ZEB9jAecr5lOz1T79EdYozxfeuPBbBncoYJAA06/QBAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJi7kap6TVW9cEePu049d6+qUVX7bW9dwN6jqk6vqmft6nbsTuwkdyNjjON3xrjA7qeqRpJ7jTE+txPqvnuSLyTZf4zxjR1d/y2VM8zdRFXtu6vbAHuzPe0qyp7W3lsCgbmTVdV950sbG6vqU1X1A/Pw11fVH1fVqVV1VZLvmYedtDDtCVV1UVVdWFXPmi+dHr0w/Unz34+oqi9W1S9V1VfmaZ6xUM/jquofquryqrqgqk68eZcC7BpVdW5VPb+qzkxyVVU9rKo+Om+Pn6yqRyyMe9uqet28vV1WVe9eKHt2VX2uqr5WVe+tqiMXykZVHV9V/zJP90dVVXPZ0VV1RlVtqqpLq+pt8/D/O0/+yaq6sqp+bGE7fn5VXZzkdVX19Kr68BafaXE/cFBV/V5VnTfP48NVdVCSlfo3zvUfM4//U1V19tzOD1TV3RbqfXRVfWau51VJagd9DXsNgbkTVdX+Sd6X5INJ7pDk55K8qaruM4/y5CQvSXKbJFtuFI9J8rwkj0pydJLvXmd2d0pyaJK7JHlmkj+qqsPnsquSPC3JYUkel+RnquqHtuOjwZ7kxzOt9/dI8p4kJyW5bZJfTvLOqrr9PN5fJLlVkvtn2l5/P0mq6nuT/HaSJya5c5Lzkrx1i3l8f5KHJHngPN6x8/AXZ9r+D0/yTUn+MEnGGN81lz9wjHHwGONt8/s7zW27W5Kfbny2303y4CT/cZ7uhCSbk6zUf9hc/9/M2/yvJ/mRJLdP8qEkb5k/4xFJ3pnkBUmOSHJOku9szP8WRWDuXA9NcnCSl40xvj7G+Osk78+0ASfJe8YYHxljbB5jXLvFtE9M8roxxqfGGFcn+a115nV9kheNMa4fY5ya5Mok90mSMcbpY4yz5vmcmWkjWS+AYW/xyjHGBUl+IsmpY4xT523hr5J8IslxVXXnJI9NcvwY47J5Ozpjnv4pSV47xvj7McZ1SX4tyTHzfcIVLxtjbBxjnJ/ktCQPmodfnyn8jhxjXDvGuMmB8So2J/nNMcZ1Y4xr1hqxqvZJ8lNJfn6M8aUxxg1jjI/ObVzNc5L89hjj7Pm+5kuTPGg+yzwuyafHGO8YY1yf5A+SXLxOW29xBObOdWSSC8YYmxeGnZfpLDBJLlhv2oX3a42bJF/d4ub+1ZnCOlX1HVV1WlVdUlWbkhyf6SgSbglWtp27JXnCfDl2Y1VtTPKwTGeNRyX52hjjslWmPzLTdpskGWNcmeSruXE7Tm4aLv+67WU646skfzffkvmpddp6ySoHz8sckeTATGeDHXdL8oqFz/61uW13yRb7mzHGyPr7nFscgblzXZjkqPlIcMVdk3xp/nusMe1FmS7hrDhqO9rx5iTvTXLUGOPQJK+J+xPccqxsZxck+YsxxmELr1uPMV42l922qg5bZfoLM4VNkqSqbp3kdrlxO14+4zEuHmM8e4xxZKYzvFev3H9cp60rrsp0mXhl3ndaKLs0ybVJ7tmoJ5k+43O2+PwHjTE+mml/86/7mPke7Pbsc/ZKAnPn+limFf6Eqtp/7mDw+Pzb+x+reXuSZ9TUaehWSX5jO9pxm0xHz9dW1bdnuncKtzRvTPL4qjq2qvatqgPnjjbfNMa4KMlfZgq0w+ftdeU+4JszbYsPqqoNmS5lfmyMce56M6yqJ1TVyoHvZZmC7Ib5/Zcz3VddyyeT3H+e94FJTlwpmK9cvTbJf6uqI+fPdMzcxksyXd5drP81SX6tqu4/t+3QqnrCXHbKPJ8fqal37n/JdD+VBQJzJxpjfD3JD2S6N3JpklcnedoY4zONaf8yySsz3Q/5XJK/mYuW3Z9Yy88meVFVXZEpeN++DXXAHm2+j/mDmTq+XJLpjOtXcuN+8KmZ7jl+JslXkvzCPN3/SfLCTJ1iLsp0Rvek5mwfkuRjVXVlpqs8Pz/G+MJcdmKSN8yXSJ+4pM2fTfKiJP87yb9ki86BmTounZXk45kusb48yT5zv4eXJPnIXP9Dxxj/cy5/a1VdnuSfMu2bMsa4NMkTkrws0+XmeyX5SPMz3mLUdKma3V1V3TfTCr7Bg8gANz9nmLuxqvrhqjpgfjzk5UneJywBdg2BuXt7TqZLR+dkuu/xM7u2OQC3XC7JAkCDM0wAaBCYANCw3q/hu14LPXvCD0HYnrlF2bx589Ky5z73uUvLXv3qV6+6PTvDBIAGgQkADQITABoEJgA0CEwAaBCYANCw3mMlALBHuvrqq5eWnXHGGVtdnzNMAGgQmADQIDABoEFgAkCDwASABoEJAA0eKwFgr7Rx48alZQcddNBW1+cMEwAaBCYANAhMAGgQmADQIDABoEEvWQB2C5s3b15ats8+W39+94pXvGJp2Wc+85mtrs8ZJgA0CEwAaBCYANAgMAGgQWACQIPABICGGmOsVb5mIexqVbWrm5AkGWPsHg1Zw7XXXrt0ez7wwANvzqZwC3b99dcvLbvhhhuWlm3YsGGr53XAAQcsLfvGN76xtGzZ9uwMEwAaBCYANAhMAGgQmADQIDABoEFgAkDDmv9bybZ22V/nURVgFzjooIOWlu233/JdwXXXXbfq8G353yNgrcdK1vrfStZ6DOTyyy/f6mm2hTUeABoEJgA0CEwAaBCYANAgMAGgQWACQMOaj5Vsq215HMWjKCyzu/yPJHuztbrf77vvvqsOv8c97rF0mnPOOWdp2Vrbuu9677etj46stW4cc8wx29WmLmeYANAgMAGgQWACQIPABIAGgQkADTull+y28EPvsGf5/Oc/v7Rsre355JNPXlr21Kc+dbvaxO5jWW/YtX58ff/9919advbZZy8tO//88/sN2w7OMAGgQWACQIPABIAGgQkADQITABoEJgA01Do/hLzHPrPhcZO9x57wg9xjjN2+kXvy9rxp06alZYcccsjN2BK6lv2Q+jXXXLN0mmU/9J8kxx133NKyM844o9+whmXbszNMAGgQmADQIDABoEFgAkCDwASABoEJAA27zf9WsqP5309g73HooYdu03TXXnvt0rINGzZsa3PYDl//+teXlr3hDW9YWrajHx3ZFs4wAaBBYAJAg8AEgAaBCQANAhMAGgQmADTstY+VbKtteRzFoyg7xp7wv5KwZznwwAOXli1b39Z67GG//ewyF6217zvrrLNWHX7KKacsneaFL3zhdrdpZ3KGCQANAhMAGgQmADQITABoEJgA0KDL1w7gh95hz7Ns+9t///23qb6zzz57adm9733vVYfvs8/uf87y2c9+dmnZJz7xiaVlL3/5y1cdfuaZZ253m3aV3f/bAoDdgMAEgAaBCQANAhMAGgQmADQITABoqLUebagqzz3sZvb0R1H21h9YH2Ps9h/M9rz7Wfa4SZKccMIJS8tudatbLS27+OKLt7odp5122tKyTZs2bXV9SfLhD3941eGbN2/epvpuTsu2Z2eYANAgMAGgQWACQIPABIAGgQkADQITABo8VgI7gMdKuDmt9XjWAQccsLRsw4YNqw5f639NWavsqquuWlp23XXXLS3b3XmsBAC2g8AEgAaBCQANAhMAGgQmADQITABo8FgJ7AAeK2FPttZjKms9VnLDDTfsjObsch4rAYDtIDABoEFgAkCDwASABoEJAA377eoGALBrrfW0xN7aE3ZbOMMEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoEJgA0CAwAaBBYAJAg8AEgAaBCQANAhMAGgQmADQITABoqDHGrm4DAOz2nGECQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABIAGgQkADQITABoEJgA0CEwAaBCYANAgMAGgQWACQIPABICG/w/2LuSKtqQ7kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate Intersection over Union\n",
    "def iou(cur_img, original, recon):\n",
    "    path = r'data\\iou'\n",
    "    \n",
    "    for i, x in enumerate(recon):\n",
    "        for j, y in enumerate(x):\n",
    "            if y.data < 0.5:\n",
    "                recon[i,j] = 0.0\n",
    "            else:\n",
    "                recon[i,j] = 1.0\n",
    "                \n",
    "    original_flat  = original.flatten().numpy().astype(int)\n",
    "    recon_flat = recon.flatten().numpy().astype(int)\n",
    "    \n",
    "    # Jaccard Scores of positive and negative classes\n",
    "    score = jaccard_score(original_flat, recon_flat, average=None)\n",
    "    # Average Jaccard Score between both classes\n",
    "    score_micro = jaccard_score(original_flat, recon_flat, average='micro')\n",
    "    return score, score_micro\n",
    "\n",
    "    \n",
    "# Plot original image alongside its reconstruction\n",
    "def plot(cur_batch, tot_batches, original, recon, loss):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Batch : {}/{}, Batch Reconstruction Loss = {:.6f}\".format(cur_batch+1, tot_batches, loss))\n",
    "    plt.axis('off')\n",
    "    # display original\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(original)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "\n",
    "    # display reconstruction\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(recon)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Main function for visualization\n",
    "def visualize(n, batches, recons, test_losses, is_compare=False, is_iou=False):\n",
    "    count = 0\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        loss = test_losses[i]\n",
    "        batch = batches[i]\n",
    "        reconstructions = recons[i]\n",
    "        # Iterate through all examples in ith batch\n",
    "        for j in range(len(batch)):\n",
    "            # If n plots have been printed, exit\n",
    "            if count >= n:\n",
    "                return\n",
    "            # Reshape original example for plotting back into 30x30\n",
    "            # or keep as vector of components if using PCA.\n",
    "            if is_pca:\n",
    "                original = batch[j].reshape(1, n_features)\n",
    "            else:\n",
    "                original = batch[j].reshape(data.shape[1], data.shape[2])\n",
    "            original = original.cpu()\n",
    "            # Reshape reconstructed example for plotting\n",
    "            # or keep as vector of components if using PCA.\n",
    "            if is_pca:\n",
    "                recon = reconstructions[j].reshape(1, n_features)\n",
    "            else:\n",
    "                recon = reconstructions[j].reshape(data.shape[1], data.shape[2])\n",
    "            recon = recon.cpu()\n",
    "            \n",
    "            if is_iou:\n",
    "                score, score_micro = iou(count, original, recon)\n",
    "                scores.append(score_micro)\n",
    "                print(f'Jaccard Similarity (Pos. & Neg.): {score}')\n",
    "                print(f\"Jaccard Similarity (Both avg'd): {score_micro}\")\n",
    "            if is_compare:\n",
    "                # print(original)\n",
    "                plot(i, len(recons), original, recon, loss)\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "visualize(n=5, batches=batches, recons=recons, test_losses=test_losses, is_compare=True, is_iou=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigan] *",
   "language": "python",
   "name": "conda-env-bigan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
