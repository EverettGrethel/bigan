{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c44b4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import cv2\n",
    "from PIL import Image as im\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "import collections\n",
    "from typing import DefaultDict, Tuple, List, Dict\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "495aef96-9947-4968-a7ac-177f81a83dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('early-stopping-pytorch')\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "# Adjust printing view dimensions\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=300)\n",
    "torch.set_printoptions(threshold=sys.maxsize, linewidth=300, profile='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d6b7c31-6d35-4067-ba20-89b11ac6b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_shape = kwargs[\"input_shape\"]\n",
    "        # number of hidden units in first hidden layer\n",
    "        self.n_units = kwargs[\"n_units\"]\n",
    "        # number of hidden units in latent space\n",
    "        self.latent_units = kwargs[\"latent_units\"]\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=self.input_shape, out_features=self.n_units),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        # Bottleneck is actually in the encoder, but it must be isolated in order to calculate sparsity\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=self.n_units, out_features=self.latent_units),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_units, out_features=self.n_units),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Linear(in_features=self.n_units, out_features=self.input_shape),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    # X denotes features\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        bottleneck = self.bottleneck(encoded)\n",
    "        decoded = self.decoder(bottleneck)\n",
    "        return bottleneck, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dca68d50-8eeb-4d3a-8150-f5508bdf9d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# 10,000 samples, 30x30 matrices\n",
    "is_pca = False\n",
    "data_count = 10000\n",
    "data = np.ndarray(shape=(data_count,30,30))\n",
    "n_features = data.shape[1] * data.shape[2]\n",
    "\n",
    "\n",
    "for i in range(data_count):\n",
    "    path = f'data/jet_matrices/sample{i+1}.dat'\n",
    "    sample = np.loadtxt(path, unpack = False)\n",
    "    data[i] = sample\n",
    "\n",
    "print(\"Done loading data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bdfa233-d473-4cab-b350-a01415b07f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading parameters.\n"
     ]
    }
   ],
   "source": [
    "# Load parameters corresponding to the 4 variables input into \n",
    "# the Helmholtz Resonator function, where output is each sample in dataset.\n",
    "params = np.ndarray(shape=(data_count,4))\n",
    "\n",
    "path = r'data/param_lhs.dat'\n",
    "with open(path) as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i >= params.shape[0]:\n",
    "            break\n",
    "        param = np.fromstring(line, dtype=float, sep=',')\n",
    "        params[i] = param\n",
    "\n",
    "print(\"Done loading parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8891f7fc-06ec-4a54-9e54-3189e67da3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flatten data and convert to Torch Tensor\n",
    "\n",
    "# 10,000 samples, 900 features\n",
    "X = np.ndarray(shape=(data_count, n_features))\n",
    "for i, sample in enumerate(data):\n",
    "    if i >= X.shape[0]:\n",
    "        break\n",
    "    flat = sample.flatten()\n",
    "    X[i] = flat\n",
    "\n",
    "# Convert from numpy array to Pytorch tensor\n",
    "X = torch.from_numpy(X)\n",
    "# Convert all scalars to floats. May affect training behavior (ie. reconstructions made of non-binary scalar values)\n",
    "X = X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d27cc79-8f11-48c0-a107-c64080b83cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair data samples with their corresponding parameter\n",
    "# in order to keep organized during random splitting.\n",
    "X_with_params = []\n",
    "for i in range(data_count):\n",
    "    pair = [X[i], params[i]]\n",
    "    X_with_params.append(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40435a6-d2d2-4a67-8b9d-a4ceeabda9f9",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38354ca1-cb26-48dc-ad4b-73c616477bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def de_correlate_data(X):\n",
    "    X_pert = np.copy(X)\n",
    "    i = 0\n",
    "    for col in X.T:\n",
    "        #print(col)\n",
    "        X_pert[:,i] = np.random.permutation(col)\n",
    "        #print(X_pert[:,i])\n",
    "        i += 1\n",
    "        \n",
    "    return X_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f802-706f-4e61-b59a-1b5a7c5c11c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot cumulative explained variance w.r.t. number of components\n",
    "\n",
    "def pca_run(X):\n",
    "    pca = PCA(n_components=0.95).fit(X)\n",
    "\n",
    "    #% matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    y = np.cumsum(pca.explained_variance_ratio_)\n",
    "    # n_components = number of components needed to reach cum. variance threshold\n",
    "    n_components = y.size\n",
    "    xi = np.arange(1, n_components+1, step=1)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    #change from 0-based array index to 1-based human-readable label\n",
    "    plt.xticks(np.arange(0, n_components+1, step=1))\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The Number of Components Needed to Explain Variance')\n",
    "\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.axhline(y=0.8, color='g', linestyle='-')\n",
    "    plt.axhline(y=0.9, color='b', linestyle='-')\n",
    "    plt.text(0, 0.915, '95% cut-off threshold', color = 'red', fontsize=13)\n",
    "    plt.text(24, 0.85, '90% cut-off threshold', color = 'blue', fontsize=13)\n",
    "    plt.text(12, 0.75, '80% cut-off threshold', color = 'green', fontsize=13)\n",
    "\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()\n",
    "\n",
    "# Run with original data.\n",
    "pca_run(X.numpy())\n",
    "\n",
    "# Run with permutated data.\n",
    "# De-correlates features, so performing worse than original data indicates\n",
    "# existence of correlation in the original data's features.\n",
    "X_pert = de_correlate_data(X)\n",
    "pca_run(X_pert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75306b-57ab-407a-bb35-d7f160e75c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(xi, pca.explained_variance_ratio_, width=0.4)\n",
    "plt.ylabel(\"Percent of Total Variance\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.title(\"Significance of Each Principal Component Towards Variance \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96362bf4-42d8-49b8-a6fa-aeb5a8359ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "# Toggle to indicate to training that PCA is in use\n",
    "is_pca = True\n",
    "# -- DEFINE NUMBER OF COMPONENTS HERE --\n",
    "n_components = 5\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(X.numpy())\n",
    "\n",
    "print(X)\n",
    "# If fails, re-run \"Flatten data...\" cell\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca = torch.from_numpy(X_pca)\n",
    "# Convert all scalars to floats. May affect training behavior (ie. reconstructions made of non-binary scalar values)\n",
    "X_pca = X_pca.float()\n",
    "# Replace former n_features with number of components\n",
    "n_features = X_pca.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875d4ed-f4c5-4bf5-878e-ccb76d4645d7",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc40b83a-dcd0-457f-9b37-0261177fb051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# Changes X based on whether PCA was used\n",
    "if is_pca:\n",
    "    X_2 = X_pca\n",
    "else:\n",
    "    X_2 = X\n",
    "\n",
    "batch_size = 32\n",
    "# 70/15/15 split\n",
    "train_size = int(0.7 * len(X_2))\n",
    "val_test_size = len(X_2) - train_size\n",
    "test_size = val_test_size // 2\n",
    "    \n",
    "val_size = val_test_size - test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d9f72fd-043f-4203-ba7e-3098c76b6a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initate data loaders\n",
    "\n",
    "train, val = torch.utils.data.random_split(X_with_params, [train_size, val_test_size], generator=torch.Generator().manual_seed(5))\n",
    "val, test = torch.utils.data.random_split(val, [val_size, test_size], generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# Same as test_loader but with stochastic batch size\n",
    "test_loader_stoch = torch.utils.data.DataLoader(\n",
    "    test, batch_size=1, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e006349d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################    \n",
    "#   TRAINING & VALIDATION   #\n",
    "#############################\n",
    "\n",
    "class ExceededRangeError(Exception):\n",
    "    \"\"\"Raised when values outside range [0.0, 1.0] are found in BCE loss\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def kl_divergence(p, p_hat):\n",
    "    # funcs = nn.Sigmoid()\n",
    "    # p_hat = torch.mean(funcs(p_hat), 1)\n",
    "    p_hat = torch.sum(p_hat, dim=0, keepdim=True)\n",
    "    p_hat = torch.nn.functional.softmax(p_hat, dim=1)\n",
    "    p_tensor = torch.Tensor([p] * len(p_hat)).to(device)\n",
    "    p_tensor = torch.nn.functional.softmax(p_tensor, dim=1)\n",
    "    return torch.sum(p_tensor * torch.log(p_tensor) - p_tensor * torch.log(p_hat) + (1 - p_tensor) * torch.log(1 - p_tensor) - (1 - p_tensor) * torch.log(1 - p_hat))\n",
    "\n",
    "\n",
    "def sparse_loss_bottle(model, rho, data):\n",
    "    loss = 0\n",
    "    layer_btl = list(model.bottleneck.children())[0]\n",
    "    rho_hat = layer_btl(data)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sparse_loss_all(model, rho, data):\n",
    "    loss = 0\n",
    "    # As rho_hat is passed through each layer, it is redefined as that layer's output.\n",
    "    rho_hat = data\n",
    "    \n",
    "    layer_enc = list(model.encoder.children())[0]\n",
    "    rho_hat = layer_enc(rho_hat)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "    \n",
    "    layer_btl = list(model.bottleneck.children())[0]\n",
    "    rho_hat = layer_btl(rho_hat)\n",
    "    loss += kl_divergence(rho, rho_hat)\n",
    "    \n",
    "    for i in range(2):\n",
    "        layer_dec = list(model.decoder.children())[2 * i]\n",
    "        rho_hat = layer_dec(rho_hat)\n",
    "        loss += kl_divergence(rho, rho_hat)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "class TrainedModel():\n",
    "    def __init__(self, model, avg_train_loss, avg_val_loss, epochs):\n",
    "        self.model = model\n",
    "        self.avg_train_loss = avg_train_loss\n",
    "        self.avg_val_loss = avg_val_loss\n",
    "        self.epochs = epochs\n",
    "\n",
    "        \n",
    "# Training and Validation are combined in order to allow for early stopping\n",
    "def train_validate(model, epochs, lr, is_early_stopping=False, is_pca=False, is_sparse=False, patience=None, beta=None, rho=None):\n",
    "    # Define Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Binary Cross Entropy Loss\n",
    "    criterion = nn.BCELoss()\n",
    "    # See pytorch docs on why reduction is batch mean.\n",
    "    kl_divergence = nn.KLDivLoss(reduction=\"batchmean\", log_target=False)\n",
    "\n",
    "    # Reset model state if previously trained\n",
    "    torch.manual_seed(1)\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            print(\"existing instance\")\n",
    "                                 \n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # Toggle Early Stopping (if using).\n",
    "    if is_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "        print(\"Using Early Stopping\")\n",
    "    if is_pca:\n",
    "        print(\"Using PCA\")\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #############################    \n",
    "        #          TRAINING         #\n",
    "        #############################\n",
    "        \n",
    "        loss = 0\n",
    "        # Prepare model for training\n",
    "        model.eval()\n",
    "        train_losses = []\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            # remove params, keep data\n",
    "            batch = batch[0]\n",
    "            # reshape mini-batch data from [batch_size, 30, 30] to [batch_size, 900]\n",
    "            # load it to the active device\n",
    "            batch = batch.view(-1, n_features).to(device)\n",
    "\n",
    "            # reset the gradients back to zero\n",
    "            # PyTorch accumulates gradients on subsequent backward passes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute reconstructions\n",
    "            # also retrieve bottleneck weights for computing sparsity penalty\n",
    "            bottleneck, decoded = model(batch)\n",
    "\n",
    "            # Exception handler for when BCE loss has values outside range [0.0, 1.0]\n",
    "            try:\n",
    "                # compute training reconstruction loss\n",
    "                train_loss = criterion(decoded, batch)\n",
    "            except RuntimeError:\n",
    "                print('Runtime Error during loss calculation. BCE loss has values outside range [0.0, 1.0]')\n",
    "                for k, sample in enumerate(decoded):\n",
    "                    print(k)\n",
    "                    print(sample)\n",
    "                    \n",
    "            # add sparsity penalty to loss, if toggled\n",
    "            if is_sparse:\n",
    "                kl_loss = sparse_loss_all(model, rho, batch)\n",
    "                sparsity_penalty = beta * kl_loss\n",
    "                train_loss = train_loss + sparsity_penalty\n",
    "                \n",
    "                # Check whether KL divergence is behaving correctly (ie. should be nonnegative).\n",
    "                if torch.all(sparsity_penalty < 0):\n",
    "                    print('Training Error: sparsity penalty is negative.')\n",
    "                    print(f'sparsity: {sparsity_penalty}\\n')\n",
    "                    print(f'training loss: {train_loss}\\n')\n",
    "\n",
    "            # compute accumulated gradients\n",
    "            train_loss.backward()\n",
    "\n",
    "            # perform parameter update based on current gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # add the mini-batch training loss to epoch loss\n",
    "            train_losses.append(train_loss.item())\n",
    "\n",
    "        # compute the epoch training loss\n",
    "        avg_train_loss = np.average(train_losses)\n",
    "\n",
    "        #############################    \n",
    "        #         VALIDATION        #\n",
    "        #############################\n",
    "\n",
    "        # Decoupled into three lists due to issue with placing torch tensors into multidimensional lists\n",
    "        batches = []\n",
    "        recons = []\n",
    "        val_losses = []\n",
    "\n",
    "        # Prepare model for evaluation\n",
    "        model.eval()\n",
    "\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader, 0):\n",
    "                # remove params, keep data \n",
    "                batch = batch[0]\n",
    "                batch = batch.view(-1, n_features).to(device)\n",
    "                bottleneck, reconstructions = model(batch)\n",
    "                # Reconstruction loss\n",
    "                val_loss = criterion(reconstructions, batch)\n",
    "                \n",
    "                # Store samples, predictions, and loss for visualization purposes\n",
    "                batches.append(batch)\n",
    "                recons.append(reconstructions)\n",
    "                val_losses.append(val_loss.item())\n",
    "                #print(f'Batch {i}: {val_loss.item()}')\n",
    "\n",
    "        avg_val_loss = np.average(val_losses)\n",
    "        \n",
    "        # display the epoch training loss and validation loss\n",
    "        print(\"Epoch : {}/{}, Training Loss = {:.6f}, Validation Loss = {:.6f}\".format(epoch + 1, epochs, avg_train_loss, avg_val_loss))\n",
    "        \n",
    "        opt_epochs = epochs\n",
    "        \n",
    "        if is_early_stopping:\n",
    "            early_stopping(avg_val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                opt_epochs = epoch + 1\n",
    "                print(\"Early stopping...\")\n",
    "                # Exit training loop\n",
    "                break\n",
    "        else:\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "    \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    print(f\"Epochs: {opt_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "    trained_model = TrainedModel(model, avg_train_loss, avg_val_loss, epochs)\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe00d81-b453-42f0-9960-31b097315258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BASIC AUTOENCODER Execute training & validating\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 5\n",
    "# number of hidden units in encoder hidden layer\n",
    "n_units = 50\n",
    "# number of hidden units in latent space\n",
    "latent_units = 4\n",
    "# Boolean for whether to use Early Stopping\n",
    "is_early_stopping = False\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "\n",
    "basic_model = AutoEncoder(input_shape=n_features,\n",
    "                    n_units=n_units,\n",
    "                    latent_units=latent_units\n",
    "                   ).to(device)\n",
    "\n",
    "basic_trained = train_validate(model=basic_model,\n",
    "                            epochs=epochs,\n",
    "                            lr=lr,\n",
    "                            is_early_stopping=is_early_stopping, \n",
    "                            is_pca=is_pca,\n",
    "                            patience=patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18dd5d37-5fac-4458-a85f-c78519f2d69d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing instance\n",
      "existing instance\n",
      "existing instance\n",
      "existing instance\n",
      "Using Early Stopping\n",
      "Training...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     14\u001b[0m sparse_model \u001b[38;5;241m=\u001b[39m AutoEncoder(input_shape\u001b[38;5;241m=\u001b[39mn_features,\n\u001b[0;32m     15\u001b[0m                     n_units\u001b[38;5;241m=\u001b[39mn_units,\n\u001b[0;32m     16\u001b[0m                     latent_units\u001b[38;5;241m=\u001b[39mlatent_units\n\u001b[0;32m     17\u001b[0m                    )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 19\u001b[0m trained_sparse \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mis_early_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_early_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mis_pca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pca\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mis_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mrho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36mtrain_validate\u001b[1;34m(model, epochs, lr, is_early_stopping, is_pca, is_sparse, patience, beta, rho)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# add sparsity penalty to loss, if toggled\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sparse:\n\u001b[1;32m--> 120\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[43msparse_loss_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     sparsity_penalty \u001b[38;5;241m=\u001b[39m beta \u001b[38;5;241m*\u001b[39m kl_loss\n\u001b[0;32m    122\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m+\u001b[39m sparsity_penalty\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36msparse_loss_all\u001b[1;34m(model, rho, data)\u001b[0m\n\u001b[0;32m     33\u001b[0m layer_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mchildren())[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     34\u001b[0m rho_hat \u001b[38;5;241m=\u001b[39m layer_enc(rho_hat)\n\u001b[1;32m---> 35\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mkl_divergence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m layer_btl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mbottleneck\u001b[38;5;241m.\u001b[39mchildren())[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     38\u001b[0m rho_hat \u001b[38;5;241m=\u001b[39m layer_btl(rho_hat)\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36mkl_divergence\u001b[1;34m(p, p_hat)\u001b[0m\n\u001b[0;32m     14\u001b[0m p_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(p_hat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m p_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([p] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(p_hat))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m p_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(p_tensor \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(p_tensor) \u001b[38;5;241m-\u001b[39m p_tensor \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(p_hat) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p_tensor) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p_tensor) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p_tensor) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p_hat))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigan\\lib\\site-packages\\torch\\nn\\functional.py:1583\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1581\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1583\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# SPARSE AUTOENCODER Execute training & validating\n",
    "lr = 1e-3\n",
    "epochs = 200\n",
    "n_units = 50\n",
    "latent_units = 20\n",
    "is_early_stopping = True\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "\n",
    "is_sparse = True\n",
    "beta = 2\n",
    "rho = 0.1\n",
    "\n",
    "sparse_model = AutoEncoder(input_shape=n_features,\n",
    "                    n_units=n_units,\n",
    "                    latent_units=latent_units\n",
    "                   ).to(device)\n",
    "\n",
    "trained_sparse = train_validate(model=sparse_model,\n",
    "                                epochs=epochs,\n",
    "                                lr=lr,\n",
    "                                is_early_stopping=is_early_stopping, \n",
    "                                is_pca=is_pca,\n",
    "                                is_sparse=is_sparse,\n",
    "                                patience=patience,\n",
    "                                beta=beta,\n",
    "                                rho=rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2a6ec0b-3e93-481f-9ed3-61ce51d04598",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH =r'C:\\Users\\evere\\Documents\\research\\bigan\\models'\n",
    "torch.save(trained_sparse.model.state_dict(), os.path.join(PATH, 'sparse_stoch_1.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834a8e2-4918-4a96-abfe-f4860ead4a29",
   "metadata": {},
   "source": [
    "# Helmholtz-Latent Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2dd8f6fc-ac70-4663-b8d6-7699cd49e223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0374, device='cuda:0'),\n",
       " tensor(0.1378, device='cuda:0'),\n",
       " tensor(-0.2029, device='cuda:0'),\n",
       " tensor(0.2914, device='cuda:0'),\n",
       " tensor(0.1806, device='cuda:0'),\n",
       " tensor(0.2087, device='cuda:0'),\n",
       " tensor(0.0275, device='cuda:0'),\n",
       " tensor(0.0709, device='cuda:0'),\n",
       " tensor(0.1019, device='cuda:0'),\n",
       " tensor(-0.0155, device='cuda:0'),\n",
       " tensor(-0.3184, device='cuda:0'),\n",
       " tensor(0.0581, device='cuda:0'),\n",
       " tensor(0.3740, device='cuda:0'),\n",
       " tensor(0.1830, device='cuda:0'),\n",
       " tensor(0.1115, device='cuda:0'),\n",
       " tensor(0.2136, device='cuda:0'),\n",
       " tensor(0.0975, device='cuda:0'),\n",
       " tensor(0.1656, device='cuda:0'),\n",
       " tensor(0.0982, device='cuda:0'),\n",
       " tensor(0.0012, device='cuda:0')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get average latent layer weights\n",
    "avg_weights = []\n",
    "for neuron in sparse_model.state_dict()['bottleneck.0.weight']:\n",
    "    avg = torch.mean(neuron)\n",
    "    avg_weights.append(avg)\n",
    "avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a0874-95fc-4c4d-8c7e-b1f2dec888e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_activations(\n",
    "        activations: DefaultDict,\n",
    "        name: str,\n",
    "        module: nn.Module,\n",
    "        inp: Tuple,\n",
    "        out: torch.Tensor\n",
    ") -> None:\n",
    "    \"\"\"PyTorch Forward hook to save outputs at each forward\n",
    "    pass. Mutates specified dict objects with each fwd pass.\n",
    "    \"\"\"\n",
    "    activations[name].append(out.detach().cpu())\n",
    "    \n",
    "\n",
    "def register_activation_hooks(\n",
    "        model: nn.Module,\n",
    "        layers_to_save: List[str]\n",
    ") -> DefaultDict[List, torch.Tensor]:\n",
    "    \"\"\"Registers forward hooks in specified layers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        PyTorch model\n",
    "    layers_to_save:\n",
    "        Module names within ``model`` whose activations we want to save.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    activations_dict:\n",
    "        dict of lists containing activations of specified layers in\n",
    "        ``layers_to_save``.\n",
    "    \"\"\"\n",
    "    activations_dict = collections.defaultdict(list)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name in layers_to_save:\n",
    "            module.register_forward_hook(\n",
    "                partial(save_activations, activations_dict, name)\n",
    "            )\n",
    "    return activations_dict\n",
    "\n",
    "\n",
    "# Save activations per layer per sample\n",
    "def get_activations(model):\n",
    "    # Enter which layers to retrieve activations from\n",
    "    to_save = ['bottleneck.0']\n",
    "\n",
    "    # register fwd hooks in specified layers\n",
    "    saved_activations = register_activation_hooks(model, layers_to_save=to_save)\n",
    "    activations_with_params = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Evaluate one sample at a time\n",
    "        for i, sample in enumerate(test_loader_stoch, 0):\n",
    "            # Remove params, keep data\n",
    "            params = sample[1]\n",
    "            sample = sample[0]\n",
    "            # move to device\n",
    "            sample = sample.to(device)\n",
    "            bottleneck, reconstruction = model(sample)\n",
    "            \n",
    "            # keep track of which activations correspond with which parameters\n",
    "            pair = [saved_activations['bottleneck.0'][i], params]\n",
    "            activations_with_params.append(pair)\n",
    "            \n",
    "    return activations_with_params\n",
    "\n",
    "\n",
    "def plot_correlations(activations_with_params):\n",
    "    for pair in activations_with_params:\n",
    "        activations = pair[0]\n",
    "        params = pair[1]\n",
    "        for i in range(activations.shape[1]):\n",
    "            print(activations[0][i])\n",
    "            for j in range(params.shape[1]):\n",
    "                print(params[0][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf51a6-191f-4ee6-9d79-07afb1d3010d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activations_with_params = get_activations(basic_model)\n",
    "# print(activations_with_params)\n",
    "\n",
    "plot_correlations(activations_with_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ff135-bccb-4e15-938f-98ceaf60be72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b25a7fcd-6d6b-4233-a89d-a446f57a2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, n_features):\n",
    "    # Decoupled into three lists due to issue with placing torch tensors into multidimensional lists\n",
    "    batches = []\n",
    "    recons = []\n",
    "    test_losses = []\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Prepare model for evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader, 0):\n",
    "            # remove params, keep data\n",
    "            batch = batch[0]\n",
    "            # move data to device\n",
    "            batch = batch.to(device)\n",
    "            bottleneck, reconstructions = model(batch)\n",
    "            # Reconstruction loss\n",
    "            test_loss = criterion(reconstructions, batch)\n",
    "            # Store samples, predictions, and loss for visualization purposes\n",
    "            batches.append(batch)\n",
    "            recons.append(reconstructions)\n",
    "            test_losses.append(test_loss.item())\n",
    "            print(f'Batch {i}: {test_loss.item()}')\n",
    "\n",
    "    avg_test_loss = np.average(test_losses)\n",
    "    print(f\"Average Test Reconstruction Loss: {avg_test_loss}\")\n",
    "    \n",
    "    return batches, recons, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac96cd0-4595-40f2-9b59-37ffc74b485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC MODEL TEST\n",
    "batches, recons, test_losses = test(basic_model, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e14ca1c5-3d5e-4f1f-874b-f586ec9cc44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: 0.004682924598455429\n",
      "Batch 1: 0.0032977189403027296\n",
      "Batch 2: 0.00411379337310791\n",
      "Batch 3: 0.0066811381839215755\n",
      "Batch 4: 0.003906615544110537\n",
      "Batch 5: 0.0036040714476257563\n",
      "Batch 6: 0.0025161877274513245\n",
      "Batch 7: 0.002733304863795638\n",
      "Batch 8: 0.003024293342605233\n",
      "Batch 9: 0.0049107796512544155\n",
      "Batch 10: 0.003772978438064456\n",
      "Batch 11: 0.003428771859034896\n",
      "Batch 12: 0.004971505608409643\n",
      "Batch 13: 0.003830864792689681\n",
      "Batch 14: 0.003739780979231\n",
      "Batch 15: 0.004062961786985397\n",
      "Batch 16: 0.002986917505040765\n",
      "Batch 17: 0.002544783754274249\n",
      "Batch 18: 0.002837176900357008\n",
      "Batch 19: 0.004262673202902079\n",
      "Batch 20: 0.004269666504114866\n",
      "Batch 21: 0.008003187365829945\n",
      "Batch 22: 0.007630040403455496\n",
      "Batch 23: 0.0035172831267118454\n",
      "Batch 24: 0.0032783427741378546\n",
      "Batch 25: 0.0044377511367201805\n",
      "Batch 26: 0.002979929791763425\n",
      "Batch 27: 0.003960439004004002\n",
      "Batch 28: 0.0021068169735372066\n",
      "Batch 29: 0.0027962166350334883\n",
      "Batch 30: 0.0031284268479794264\n",
      "Batch 31: 0.0031489122193306684\n",
      "Batch 32: 0.002787240082398057\n",
      "Batch 33: 0.003373104380443692\n",
      "Batch 34: 0.004115738440304995\n",
      "Batch 35: 0.004033754114061594\n",
      "Batch 36: 0.00290851597674191\n",
      "Batch 37: 0.0028368246275931597\n",
      "Batch 38: 0.00402991846203804\n",
      "Batch 39: 0.005013416055589914\n",
      "Batch 40: 0.003742828033864498\n",
      "Batch 41: 0.00243463646620512\n",
      "Batch 42: 0.007811963092535734\n",
      "Batch 43: 0.004696634132415056\n",
      "Batch 44: 0.0029630677308887243\n",
      "Batch 45: 0.003056052140891552\n",
      "Batch 46: 0.00450953608378768\n",
      "Average Test Reconstruction Loss: 0.0039038188319574013\n"
     ]
    }
   ],
   "source": [
    "# SPARSE MODEL TEST\n",
    "batches, recons, test_losses = test(sparse_model, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0797b-b169-4534-a8cb-bf679aad6a30",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "411133d1-7a8f-4acf-9c70-3fb090cef252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate Intersection over Union\n",
    "def iou(cur_img, original, recon):\n",
    "    path = r'data\\iou'\n",
    "    \n",
    "    for i, x in enumerate(recon):\n",
    "        for j, y in enumerate(x):\n",
    "            if y.data < 0.5:\n",
    "                recon[i,j] = 0.0\n",
    "            else:\n",
    "                recon[i,j] = 1.0\n",
    "                \n",
    "    original_flat  = original.flatten().numpy().astype(int)\n",
    "    recon_flat = recon.flatten().numpy().astype(int)\n",
    "    \n",
    "    # Jaccard Scores of positive and negative classes\n",
    "    score = jaccard_score(original_flat, recon_flat, average=None)\n",
    "    # Average Jaccard Score between both classes\n",
    "    score_micro = jaccard_score(original_flat, recon_flat, average='micro')\n",
    "    return score, score_micro\n",
    "    \n",
    "#     # convert arrays to grayscale\n",
    "#     original = np.array(original * 255, dtype = np.uint8)\n",
    "#     recon = np.array(recon * 255, dtype = np.uint8)\n",
    "    \n",
    "#     path1 = f'{path}\\original{cur_img}.png'\n",
    "#     path2 = f'{path}\\\\recon{cur_img}.png'\n",
    "    \n",
    "#     cv2.imwrite(path1, original)\n",
    "#     cv2.imwrite(path2, recon)\n",
    "    \n",
    "#     original_img = cv2.imread(path1, 0)\n",
    "#     recon_img = cv2.imread(path2, 0)\n",
    "    \n",
    "#     intersect = cv2.bitwise_and(original_img, recon_img)\n",
    "#     union = cv2.bitwise_or(original_img, recon_img)\n",
    "\n",
    "#     plt.imshow(intersect, cmap='gray', vmin=0, vmax=255)\n",
    "#     plt.axis('off')\n",
    "#     title1 = f'original{cur_img}'\n",
    "#     plt.title(title1)\n",
    "#     plt.show()\n",
    "#     title2 = f'recon{cur_img}'\n",
    "\n",
    "    \n",
    "# Plot original image alongside its reconstruction\n",
    "def plot(cur_batch, tot_batches, original, recon, loss):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Batch : {}/{}, Batch Reconstruction Loss = {:.6f}\".format(cur_batch+1, tot_batches, loss))\n",
    "    plt.axis('off')\n",
    "    # display original\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(original)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "\n",
    "    # fig.get_xaxis().set_visible(False)\n",
    "    # fig.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(recon)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    # fig.get_xaxis().set_visible(False)\n",
    "    # fig.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Main function for visualization\n",
    "def visualize(n, batches, recons, test_losses, is_compare=False, is_iou=False):\n",
    "    count = 0\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        loss = test_losses[i]\n",
    "        batch = batches[i]\n",
    "        reconstructions = recons[i]\n",
    "        # Iterate through all examples in ith batch\n",
    "        for j in range(len(batch)):\n",
    "            # If n plots have been printed, exit\n",
    "            if count >= n:\n",
    "                return\n",
    "            # Reshape original example for plotting back into 30x30\n",
    "            # or keep as vector of components if using PCA.\n",
    "            if is_pca:\n",
    "                original = batch[j].reshape(1, n_features)\n",
    "            else:\n",
    "                original = batch[j].reshape(data.shape[1], data.shape[2])\n",
    "            original = original.cpu()\n",
    "            # Reshape reconstructed example for plotting\n",
    "            # or keep as vector of components if using PCA.\n",
    "            if is_pca:\n",
    "                recon = reconstructions[j].reshape(1, n_features)\n",
    "            else:\n",
    "                recon = reconstructions[j].reshape(data.shape[1], data.shape[2])\n",
    "            recon = recon.cpu()\n",
    "            \n",
    "            if is_iou:\n",
    "                score, score_micro = iou(count, original, recon)\n",
    "                scores.append(score_micro)\n",
    "                print(f'Jaccard Similarity (Pos. & Neg.): {score}')\n",
    "                print(f\"Jaccard Similarity (Both avg'd): {score_micro}\")\n",
    "            if is_compare:\n",
    "                print(original)\n",
    "                plot(i, len(recons), original, recon, loss)\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "visualize(n=5, batches=batches, recons=recons, test_losses=test_losses, is_compare=True, is_iou=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigan] *",
   "language": "python",
   "name": "conda-env-bigan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
